{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR100 Hierarchical Classification (Tensorflow 2.2.0)\n",
    "\n",
    "    Author: Lukas Friedrichsen (friedrichsen.luk@googlemail.com)\n",
    "    License: Apache License, Version 2.0\n",
    "\n",
    "Description: In this notebook, different experiments are conducted on various scenarios from the field of image recognition to assess the viability of modularization as a technique to counteract specific inherent shortcomings of Neural Networks. In the first experiment, different performance criteria are measured on the CIFAR100 dataset for a hierarchically composed network and a comparative reference model. The second experiment demonstrated in this notebook serves to assess the effect of modularization with regard to predictive model robustness. For this, the performance of the two models from the first experiment is evaluated and compared on distributionally shifted and OOD (Out-Of-Distribution) testing data (the former modelled by the CIFAR100-C, the latter by the CIFAR10 dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Configuration](#config)\n",
    "3. [Loading the dataset](#load)\n",
    "4. [Mapping synset relationships](#synset_mapping)\n",
    "5. [Processing and augmentation](#processing_augmentation)\n",
    "6. [Composed Network (CompNet)](#compnet)\n",
    "  1. [Model](#compnet_model)\n",
    "  2. [Training](#compnet_train)\n",
    "  3. [Testing](#compnet_test)\n",
    "  4. [Predictive uncertainty under dataset shift](#compnet_predictive_uncertainty)\n",
    "7. [Benchmark: Maxout (Goodfellow et. al, 2013)](#benchmark)\n",
    "  1. [Model](#benchmark_model)\n",
    "  2. [Training](#benchmark_train)\n",
    "  3. [Testing](#benchmark_test)\n",
    "  4. [Predictive uncertainty under dataset shift](#benchmark_predictive_uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports\n",
    "<a id ='imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# Matplotlib configuration\n",
    "\n",
    "# General settings\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['grid.alpha'] = 0.5\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['legend.framealpha'] = 1.0\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# Font sizes\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['axes.titlesize'] = 15\n",
    "mpl.rcParams['figure.titlesize'] = 20\n",
    "mpl.rcParams['legend.fontsize'] = 15\n",
    "mpl.rcParams['xtick.labelsize'] = 15 \n",
    "mpl.rcParams['ytick.labelsize'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print('Tensorflow: v{}'.format(tf.__version__))\n",
    "print('Tensorflow Datasets: v{}'.format(tfds.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducability\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "<a id ='config'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and storage locations for Tensorflow Datasets\n",
    "TFDS_DATA_DIR = '/Volumes/Data/tensorflow_datasets/'\n",
    "TFDS_DOWNLOAD_DIR = '/Volumes/Data/tensorflow_datasets/downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage locations for model checkpoints and training logs\n",
    "CKPT_DIR = '.model_checkpoints/cifar100/'\n",
    "LOG_DIR = 'logs/cifar100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage location for data from experiments\n",
    "RESULTS_DIR = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specific configuration\n",
    "\n",
    "# Storage locations for data from experiments on the composed resp. benchmark network\n",
    "CIFAR100_RESULTS_DIR_COMPNET = RESULTS_DIR + 'cifar100/compnet/'\n",
    "CIFAR100_RESULTS_DIR_BENCHMARK = RESULTS_DIR + 'cifar100/benchmark/'\n",
    "\n",
    "# Files containing the string literals corresponding to the numeric labels from the CIFAR100 dataset\n",
    "# Label 'i' in the dataset corresponds to the i-th entry of the respective file\n",
    "CIFAR100_COARSE_LABELS_TO_LITERALS_FILE = TFDS_DATA_DIR + 'cifar100/1.3.1/coarse_label.labels.txt'\n",
    "CIFAR100_FINE_LABELS_TO_LITERALS_FILE = TFDS_DATA_DIR + 'cifar100/1.3.1/label.labels.txt'\n",
    "\n",
    "# Keys of the dataset fields containing images and labels\n",
    "CIFAR100_IMG_KEY = 'image'\n",
    "CIFAR100_COARSE_LABEL_KEY = 'coarse_label'\n",
    "CIFAR100_FINE_LABEL_KEY = 'label'\n",
    "\n",
    "# Synset level of the different label categories (indicating the hierarchical relation between the\n",
    "# different categories)\n",
    "CIFAR100_COARSE_LABEL_LEVEL = 0\n",
    "CIFAR100_FINE_LABEL_LEVEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing configuration\n",
    "\n",
    "# Cropping dimensions\n",
    "CROP_SIZE_H = 28\n",
    "CROP_SIZE_W = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the dataset\n",
    "<a id='load'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cifar100_train_raw, cifar100_val_raw, cifar100_test_raw], cifar100_info = tfds.load('cifar100',\n",
    "                                                                                     split=[tfds.Split.TRAIN.subsplit(tfds.percent[:80]),\n",
    "                                                                                            tfds.Split.TRAIN.subsplit(tfds.percent[-20:]),\n",
    "                                                                                            tfds.Split.TEST],\n",
    "                                                                                     data_dir=TFDS_DATA_DIR,\n",
    "                                                                                     download_and_prepare_kwargs={'download_dir': TFDS_DOWNLOAD_DIR},\n",
    "                                                                                     with_info=True)\n",
    "print(cifar100_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample image to make sure loading worked correctly\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "titles = ['Train', 'Validation', 'Test']\n",
    "\n",
    "for idx, dataset in enumerate([cifar100_train_raw, cifar100_val_raw, cifar100_test_raw]):\n",
    "    for record in dataset.take(1):\n",
    "        ax[idx].imshow(record[CIFAR100_IMG_KEY])\n",
    "        ax[idx].set_title(titles[idx])\n",
    "        ax[idx].axis('off')\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the distribution of the train, validation and test dataset to validate, that the former is\n",
    "# representative for the latter two (requires the initalization of `CIFAR100_SYNSET_MAP` prior to\n",
    "# execution (cf. below))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "titles = ['Train', 'Validation', 'Test']\n",
    "\n",
    "for idx, dataset in enumerate([cifar100_train_raw, cifar100_val_raw, cifar100_test_raw]):\n",
    "    # Get the maximum label (called 'label depth' in Tensorflow)\n",
    "    label_depth = CIFAR100_NUM_FINE_LABELS\n",
    "\n",
    "    # Initialize the label distribution\n",
    "    dist = [0] * (label_depth + 1)\n",
    "    \n",
    "    # Get the label distribution for the current dataset\n",
    "    for record in dataset:\n",
    "        dist[record['label'].numpy()] += 1\n",
    "        \n",
    "    # Normalize the distribution\n",
    "    dist = list(map(lambda entry: entry / sum(dist) * 100, dist)) \n",
    "                \n",
    "    # Plot the distribution\n",
    "    ax[idx].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[idx].set_title(titles[idx])\n",
    "    ax[idx].set_xlim([0.5, 100.5])\n",
    "    ax[idx].set_ylim([0, 0.015])\n",
    "    ax[idx].set_xlabel('Label')\n",
    "    ax[idx].set_ylabel('Share in %')\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mapping synset relationships\n",
    "<a id='synset_mapping'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create a mapping between hyper- and hyponyms (i.e. coarse and fine labels) in analogy to the wordnet corpus underlying ImageNet to be able to model the relations between the different hierarchy levels of labels and as a basis for the structure of the composed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation: We'll eventually outsource this class into a dedicated module, thus we're including\n",
    "# necessary imports, etc. here instead of putting them at the top of the notebook together with\n",
    "# the rest to keep all ressources in one place.\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class synset_map(object):\n",
    "    '''Representational model for hierarchical syntactical structures\n",
    "    \n",
    "    This class implements a representational model for (injective) hierarchical syntactical structures.\n",
    "    It provides the necessary functionalities to create a mapping between multilayered hyper- and\n",
    "    hyponym compositions as well as to trace the inherent relations as well as to measure the semantic\n",
    "    distance between different synsets.\n",
    "    \n",
    "    Args:\n",
    "        synset_map (optional): Representational model for a hierarchical syntactical structure (e.g.\n",
    "            manually constructed; takes highest priority if provided together with `dataset` and\n",
    "            `keys`)\n",
    "        dataset (optional): Dataset-like structure that can be accessed via `keys`and that contains\n",
    "            the hyper- and hyponyms whose relation is to be mapped (assuming an unambiguous, injective\n",
    "            structure of synsets)\n",
    "        keys (optional): List of keys that can be used to access the fields of `dataset` that contain\n",
    "            the synset specifiers\n",
    "    \n",
    "    Attributes:\n",
    "        synset_map (dict): Nested structure of dicts that serves to store the hierarchical relationships\n",
    "            between the different synsets\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, synset_map=None, dataset=None, keys=None):\n",
    "        if synset_map is not None:\n",
    "            self.synset_map = synset_map\n",
    "        elif (dataset is not None) and (keys is not None):\n",
    "            self.synset_map_from_dataset(dataset, keys)\n",
    "        else:\n",
    "            self.synset_map = {}\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def synset_map(self):\n",
    "        return deepcopy(self._synset_map)\n",
    "    \n",
    "    \n",
    "    @synset_map.setter\n",
    "    def synset_map(self, synset_map):\n",
    "        elements = [synset_map]\n",
    "        for element in elements:\n",
    "            if not isinstance(element, dict):\n",
    "                raise TypeError(\n",
    "                    'All entries of `synset_map` have to be of type `dict`.\\n'\n",
    "                )\n",
    "\n",
    "            if element:\n",
    "                for value in element.values():\n",
    "                    elements.append(value)\n",
    "        \n",
    "        self._synset_map = synset_map\n",
    "        \n",
    "        self.construct_hyponym_map()\n",
    "        self.construct_hypernym_map()\n",
    "        \n",
    "    \n",
    "    def construct_hyponym_map(self):\n",
    "        '''Constructs a dictionary containing the hyponyms for every synset in `synset_map`\n",
    "        \n",
    "        Constructs a dictionary containing the hyponyms for every synset in `synset_map`. This\n",
    "        function is called exactly once every time `synset_map` is set, thus reducing the complexity\n",
    "        of subsequent lookup operations to O(1). Has to be called manually if changes to an existing\n",
    "        synset map are made.\n",
    "        \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "                \n",
    "        if not self._synset_map:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` before calling this function.\\n'\n",
    "            )\n",
    "                    \n",
    "        self._hyponyms = {}\n",
    "        \n",
    "        synsets = [self._synset_map]\n",
    "        for synset in synsets:\n",
    "            for hypernym in synset.keys():                \n",
    "                self._hyponyms[hypernym] = list(synset[hypernym].keys())\n",
    "                synsets.append(synset[hypernym])\n",
    "            \n",
    "        \n",
    "    def construct_hypernym_map(self):\n",
    "        '''Constructs a dictionary containing the complete hypernym path for every synset in `synset_map`\n",
    "        \n",
    "        Constructs a dictionary containing the complete hypernym path for every synset in `synset_map`.\n",
    "        This function is called exactly once every time `synset_map`is changed, thus reducing the\n",
    "        complexity of subsequent lookup operations to O(1). Has to be called manually if changes to\n",
    "        an existing synset map are made.\n",
    "        \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._synset_map:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` before calling this function.\\n'\n",
    "            )\n",
    "        \n",
    "        self._hypernym_paths = {}\n",
    "        \n",
    "        synsets = [self._synset_map]\n",
    "        for synset in synsets:\n",
    "            for hypernym, hyponyms in zip(synset.keys(), synset.values()):\n",
    "                hypernym_path = [hypernym]\n",
    "\n",
    "                while True:\n",
    "                    # Check if the current `hypernym` is part of the root layer (meaning it has no\n",
    "                    # further hypernyms)\n",
    "                    if hypernym in self._synset_map.keys():\n",
    "                        break\n",
    "\n",
    "                    elements = [self._synset_map]\n",
    "                    for element in elements:\n",
    "                        for key in element.keys():\n",
    "                            if hypernym in element[key].keys():\n",
    "                                hypernym = key\n",
    "                                break\n",
    "                            else:\n",
    "                                elements.append(element[key])\n",
    "\n",
    "                    hypernym_path.append(hypernym)\n",
    "\n",
    "                self._hypernym_paths[hypernym_path[0]] = hypernym_path[::-1]\n",
    "                \n",
    "                synsets.append(hyponyms)\n",
    "    \n",
    "    \n",
    "    def synset_map_from_dataset(self, dataset, keys):\n",
    "        '''Creates a mapping between hyper- and hyponyms from a given dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset-like structure that can be accessed via `keys`and that contains the\n",
    "                hyper- and hyponyms whose relation is to be mapped (assuming an unambiguous, injective\n",
    "                structure of synsets)\n",
    "            keys: List of keys that can be used to access the fields of `dataset` that contain the\n",
    "                synset specifiers\n",
    "            \n",
    "        Raises:\n",
    "            TypeError: If one of the input arguments is of the wrong type\n",
    "            ValueError: If invalid values are specified for one or more input arguments\n",
    "        '''\n",
    "        \n",
    "        if not isinstance(dataset, list):\n",
    "            raise TypeError(\n",
    "                '`dataset` is expected to be of type `list`. Is of type {}.\\n'.format(type(dataset))\n",
    "            )\n",
    "        if not dataset:\n",
    "            raise ValueError(\n",
    "                '`dataset` may not be empty.\\n'\n",
    "            )\n",
    "        if not isinstance(keys, list):\n",
    "            raise TypeError(\n",
    "                '`keys` is expected to be of type `list`. Is of type {}.\\n'.format(type(keys))\n",
    "            )\n",
    "        if not keys:\n",
    "            raise ValueError(\n",
    "                '`keys` may not be empty.\\n'\n",
    "            )\n",
    "            \n",
    "        # Determine the hierarchical relationship between the given keys (i.e. which fields in the\n",
    "        # dataset contain the highest-level synsets, which ones contain the second highest and so on)\n",
    "        \n",
    "        ordered_keys = keys\n",
    "        len_ordered_keys = len(ordered_keys)\n",
    "        if len_ordered_keys > 1:\n",
    "            # Assuming an injective structure of the syntactic relationships, determine the hierarchical\n",
    "            # relationship between two values of `keys` by simply iterate over the dataset until one\n",
    "            # of the fields differs for the same value of the field referenced by the other key,\n",
    "            # indicating that the former is a hyponym of the latter. The keys can then be sorted\n",
    "            # using e.g. Bubble Sort as is done here.\n",
    "            for i in range(len_ordered_keys):\n",
    "                for j in range(0, len_ordered_keys - i - 1):\n",
    "                    key_1 = ordered_keys[j]\n",
    "                    key_2 = ordered_keys[j + 1]\n",
    "                    \n",
    "                    val_key_1 = dataset[0][key_1]\n",
    "                    val_key_2 = dataset[0][key_2]\n",
    "                        \n",
    "                    for record in dataset:\n",
    "                        if (val_key_1 != record[key_1]) and (val_key_2 == record[key_2]):\n",
    "                            ordered_keys[j], ordered_keys[j + 1] = ordered_keys[j + 1], ordered_keys[j]\n",
    "                            break\n",
    "        \n",
    "        # Construct the mapping between hierarchically related synsets from the dataset\n",
    "        \n",
    "        synset_map = {}\n",
    "        for record in dataset:\n",
    "            synset = synset_map\n",
    "            \n",
    "            for key in ordered_keys:\n",
    "                hyponym = record[key]\n",
    "                \n",
    "                if not hyponym in synset.keys():\n",
    "                    synset[hyponym] = {}\n",
    "                    \n",
    "                synset = synset[hyponym]\n",
    "        \n",
    "        self.synset_map = synset_map\n",
    "        \n",
    "    \n",
    "    def hyponyms(self, synset):\n",
    "        '''Returns the hyponyms for a given synset\n",
    "        \n",
    "        Args:\n",
    "            synset: Key / label of the synset whose hyponyms are to be returned\n",
    "        \n",
    "        Returns:\n",
    "            hyponyms: List containing the hyponyms of the given synset (empty if `synset` is not in\n",
    "                `synset_map` or if `synset` is a leaf node)\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._hyponyms:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` or call `construct_hyponym_map` before calling this function.\\n'\n",
    "            )\n",
    "            \n",
    "        hyponyms = []\n",
    "                \n",
    "        if synset in self._hyponyms.keys():\n",
    "            hyponyms = self._hyponyms[synset]\n",
    "\n",
    "        return hyponyms\n",
    "    \n",
    "    \n",
    "    def hypernym(self, synset):\n",
    "        '''Returns the hypernym for a given synset\n",
    "        \n",
    "        Args:\n",
    "            synset: Key / label of the synset whose hypernym is to be returned\n",
    "        \n",
    "        Returns:\n",
    "            hypernym: Hypernym of the given synset (`None` if `synset` is not in `synset_map` or if\n",
    "                `synset` is part of the root layer)\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._hypernym_paths:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` or call `construct_hypernym_map` before calling this function.\\n'\n",
    "            )\n",
    "        \n",
    "        hypernym = None\n",
    "           \n",
    "        if synset in self._hypernym_paths.keys():\n",
    "            # Check if `synset` is part of the root layer\n",
    "            if len(self._hypernym_paths[synset]) > 1:\n",
    "                hypernym = self._hypernym_paths[synset][-2]\n",
    "        \n",
    "        return hypernym\n",
    "    \n",
    "    \n",
    "    def hypernym_path(self, synset):\n",
    "        '''Returns the hypernym path from a given synset to the root layer\n",
    "        \n",
    "        Args:\n",
    "            synset: Key / label of the synset whose hypernym path is to be returned\n",
    "        \n",
    "        Returns:\n",
    "            hypernym_path: List containing the hypernym path in descending order from the root layer\n",
    "                down to `synset` (empty if `synset` is not in `synset_map`)\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._hypernym_paths:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` or call `construct_hypernym_map` before calling this function.\\n'\n",
    "            )\n",
    "            \n",
    "        hypernym_path = []\n",
    "        \n",
    "        if synset in self._hypernym_paths.keys():\n",
    "            hypernym_path = self._hypernym_paths[synset]\n",
    "                        \n",
    "        return hypernym_path\n",
    "    \n",
    "    \n",
    "    def is_a(self, hyponym, hypernym):\n",
    "        '''Checks whether a synset is a hyponym of another synset\n",
    "        \n",
    "        Args:\n",
    "            hyponym: Key / label of the hierarchically subordinate synset in question\n",
    "            hypernym: Key / label of the hierarchically superordinate synset in question\n",
    "        \n",
    "        Returns:\n",
    "            is_hyponym: Indicator whether `hyponym` is a hyponym of `hypernym`\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._synset_map:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` before calling this function.\\n'\n",
    "            )\n",
    "        \n",
    "        return (hypernym in self.hypernym_path(hyponym))\n",
    "    \n",
    "    \n",
    "    def semantic_distance(self, synset_1, synset_2):\n",
    "        '''Calculates the semantic distance between two synsets in accordance to (Fergus et al., 2010)\n",
    "        \n",
    "        Args:\n",
    "            hyponym: Key / label of the first synset\n",
    "            hypernym: Key / label of the second synset\n",
    "            \n",
    "        Returns:\n",
    "            semantic_dist (float): Semantic distance between the two given synsets (0.0 if one of\n",
    "                the synsets is not in `synset_map`)\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._synset_map:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` before calling this function.\\n'\n",
    "            )\n",
    "            \n",
    "        semantic_dist = 0.0\n",
    "            \n",
    "        hypernym_path_1 = self.hypernym_path(synset_1)\n",
    "        hypernym_path_2 = self.hypernym_path(synset_2)\n",
    "        \n",
    "        if hypernym_path_1 and hypernym_path_2:\n",
    "            # Semantic distance is defined by (Fergus et al., 2010) as follows:\n",
    "            # S(i, j) = intersect(path(i), path(j)) / max(length(path(i)), length(path(j)))\n",
    "            semantic_dist = len([hypernym for hypernym in hypernym_path_1 if hypernym in hypernym_path_2]) / max(len(hypernym_path_1), len(hypernym_path_2))\n",
    "        \n",
    "        return semantic_dist\n",
    "    \n",
    "    \n",
    "    def synset_level(self, synset):\n",
    "        '''Returns the level of the given synset in the syntactical structure\n",
    "        \n",
    "        Args:\n",
    "            synset: Key / label of the synset in question\n",
    "            \n",
    "        Returns:\n",
    "            level: Level of the given synset in the hierarchical structure (zero indicating root\n",
    "                level, one the first level below root level, etc.; `None` if `synset` is not in\n",
    "                `synset_map`)\n",
    "            \n",
    "        Raises:\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not self._synset_map:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` before calling this function.\\n'\n",
    "            )\n",
    "        \n",
    "        hypernym_path = self.hypernym_path(synset)\n",
    "        \n",
    "        if not hypernym_path:\n",
    "            return None\n",
    "        \n",
    "        return len(hypernym_path) - 1\n",
    "    \n",
    "    \n",
    "    def get_all_synsets_of_level(self, level):\n",
    "        '''Returns all synsets of the specified level in the hierarchical syntactic structure\n",
    "        \n",
    "        Args:\n",
    "            level (int): Positive integer specifying of level of `synset_map` from which the synsets\n",
    "                are to be returned\n",
    "        \n",
    "        Returns:\n",
    "            synset: List of all synsets on the specified level in `synset_map` (empty if `level` is\n",
    "                greater than the maximum depth of `synset_map`)\n",
    "                \n",
    "        Raises:\n",
    "            TypeError: If one of the input arguments is of the wrong type\n",
    "            ValueError: If invalid values are specified for one or more input arguments\n",
    "            UserWarning: If `synset_map` has not been initialized at call time\n",
    "        '''\n",
    "        \n",
    "        if not isinstance(level, int):\n",
    "            raise TypeError(\n",
    "                '`level` is expected to be of type `int`. Is of type {}.\\n'.format(type(level))\n",
    "            )\n",
    "        if level < 0:\n",
    "            raise ValueError(\n",
    "                '`level` has to be a positive value.\\n'\n",
    "            )\n",
    "        if not self._hypernym_paths:\n",
    "            raise UserWarning(\n",
    "                'Please initialize `synset_map` or call `construct_hypernym_map` before calling this function.\\n'\n",
    "            )\n",
    "            \n",
    "        synset = []\n",
    "        \n",
    "        for hypernym_path in self._hypernym_paths.values():\n",
    "            if len(hypernym_path) - 1 >= level:\n",
    "                if hypernym_path[level] not in synset:\n",
    "                    synset.append(hypernym_path[level])\n",
    "                \n",
    "        return synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings between (encoded) numeric and (decoded) string literal labels\n",
    "CIFAR100_COARSE_LABELS_TO_LITERALS = [literal.numpy().decode('utf8') for literal in tf.data.TextLineDataset(CIFAR100_COARSE_LABELS_TO_LITERALS_FILE)]\n",
    "CIFAR100_FINE_LABELS_TO_LITERALS = [literal.numpy().decode('utf8') for literal in tf.data.TextLineDataset(CIFAR100_FINE_LABELS_TO_LITERALS_FILE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar100_decode(label, level=CIFAR100_FINE_LABEL_LEVEL):\n",
    "    '''Returns the literal associated with the given combination of encoded label and hierarchy level\n",
    "    \n",
    "    Args:\n",
    "        label (int): Encoded label\n",
    "        level (int, optional): Level of the given label in the hierarchical structure (`0` indicating\n",
    "            coarse, `1` (default) fine labels)\n",
    "    \n",
    "    Returns:\n",
    "        decoded_label (str): String literal belonging to the given label / level combination\n",
    "    '''\n",
    "   \n",
    "    # Cut off invalid values for `level` (has to be either `0` or `1`)\n",
    "    level = max(0, min(CIFAR100_FINE_LABEL_LEVEL, level))\n",
    "    \n",
    "    if level == CIFAR100_COARSE_LABEL_LEVEL:\n",
    "        return CIFAR100_COARSE_LABELS_TO_LITERALS[label]\n",
    "   \n",
    "    return CIFAR100_FINE_LABELS_TO_LITERALS[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar100_encode(label):\n",
    "    '''Returns the combination of encoded label and hierarchy level associated with the given literal\n",
    "    \n",
    "    Args:\n",
    "        label (str): Decoded label\n",
    "    \n",
    "    Returns:\n",
    "        encoded_label (int): Numeric label belonging to the given string literal\n",
    "        level (int): Level of the `encoded_label` in the hierarchical structure (`0` indicating coarse,\n",
    "            `1` fine labels)\n",
    "    '''\n",
    "    \n",
    "    if label in CIFAR100_COARSE_LABELS_TO_LITERALS:\n",
    "        return CIFAR100_COARSE_LABELS_TO_LITERALS.index(label), CIFAR100_COARSE_LABEL_LEVEL\n",
    "        \n",
    "    return CIFAR100_FINE_LABELS_TO_LITERALS.index(label), CIFAR100_FINE_LABEL_LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation: This function serves to aggregate and encapsulate all native (non-tensorized) Python\n",
    "# functionalities that require eager execution for direct data access during the preprocessing of\n",
    "# the dataset to minimize the overhead resulting from converting TensorFlow data structures to Python\n",
    "# objects and back at runtime.\n",
    "\n",
    "def cifar100_resolve_hypernym(label, level, encoded=False):\n",
    "    '''Returns the hypernym of the given (fine) label on the specified hierarchy level\n",
    "\n",
    "    Given a label on the finest supported categorical resolution (i.e. fine labels for the CIFAR100\n",
    "    dataset) returns the related hypernym on the specified hierarchy level\n",
    "\n",
    "    Args:\n",
    "        label: Label of the synset on the finest supported categorical resolution whose hypernym is to\n",
    "            be returned in numerical or string literal format\n",
    "        level (int): Level of the given label in the hierarchical structure (`0` indicating\n",
    "            coarse, `1` (default) fine labels)\n",
    "        encoded (bool, optional): Indicator whether to return the hypernym in encoded or decoded\n",
    "            format\n",
    "\n",
    "    Returns:\n",
    "        hypernym: Hypernym of the given synset on the hierarchy level specified by `level` in numerical\n",
    "            or string literal format depending on `encoded`\n",
    "    '''\n",
    "\n",
    "    # `tf.Tensor` compatibility\n",
    "    if isinstance(label, tf.Tensor):\n",
    "        label = label.numpy()\n",
    "\n",
    "    # Decode the label if it was given in encoded format\n",
    "    if not isinstance(label, str):\n",
    "        label = cifar100_decode(label, CIFAR100_FINE_LABEL_LEVEL)\n",
    "\n",
    "    hypernym = CIFAR100_SYNSET_MAP.hypernym_path(label)[level]\n",
    "    \n",
    "    if encoded:\n",
    "        hypernym, _ = cifar100_encode(hypernym)\n",
    "\n",
    "    return hypernym\n",
    "\n",
    "# TensorFlow wrapper to be able to apply this function to placeholder object, thus being able to\n",
    "# employ it as part of the preprocessing pipeline\n",
    "def tf_cifar100_resolve_hypernym_decoded(label, level):\n",
    "    return tf.py_function(cifar100_resolve_hypernym, inp=(label, level, False), Tout=tf.dtypes.string)\n",
    "def tf_cifar100_resolve_hypernym_encoded(label, level):\n",
    "    return tf.py_function(cifar100_resolve_hypernym, inp=(label, level, True), Tout=tf.dtypes.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between hyper- and hyponyms (i.e. coarse and fine labels) of the CIFAR100 dataset\n",
    "CIFAR100_SYNSET_MAP = synset_map(\n",
    "    dataset=[{CIFAR100_COARSE_LABEL_KEY: cifar100_decode(\n",
    "                  record[CIFAR100_COARSE_LABEL_KEY].numpy(), CIFAR100_COARSE_LABEL_LEVEL),\n",
    "              CIFAR100_FINE_LABEL_KEY: cifar100_decode(\n",
    "                  record[CIFAR100_FINE_LABEL_KEY].numpy(), CIFAR100_FINE_LABEL_LEVEL)}\n",
    "             for record in cifar100_test_raw],\n",
    "    keys=[CIFAR100_COARSE_LABEL_KEY, CIFAR100_FINE_LABEL_KEY])\n",
    "\n",
    "# Macros for the number of coarse resp. fine labels\n",
    "CIFAR100_NUM_COARSE_LABELS = len(CIFAR100_SYNSET_MAP.get_all_synsets_of_level(CIFAR100_COARSE_LABEL_LEVEL))\n",
    "CIFAR100_NUM_FINE_LABELS = len(CIFAR100_SYNSET_MAP.get_all_synsets_of_level(CIFAR100_FINE_LABEL_LEVEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Processing and augmentation\n",
    "<a id='processing_augmentation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, different functions are provided for preprocessing the dataset prior to the training and inference adhering to the standard 10-crop procedure with contrast normalization and ZCA whitening as described in (Goodfellow et al., 2013), (Krizhevsky, 2009) and (Krizhevsky et al., 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop out a patch of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, offset_height, offset_width, target_height, target_width):\n",
    "    '''Simple, type-preserving wrapper around `tf.image.crop_to_bounding_box`\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        offset_height (int): Vertical coordinate of the top-left corner of the crop\n",
    "        offset_width (int): Horizontal coordinate of the top-left corner of the crop\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the cropped image\n",
    "    '''\n",
    "    \n",
    "    return tf.cast(\n",
    "        tf.image.crop_to_bounding_box(\n",
    "            image,\n",
    "            offset_height, offset_width,\n",
    "            target_height, target_width),\n",
    "        image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_central(image, target_height, target_width):\n",
    "    '''Crops the central patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the central crop of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = (height - target_height) // 2\n",
    "    offset_width = (width - target_width) // 2\n",
    "    \n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_corner_upper_left(image, target_height, target_width):\n",
    "    '''Crops the upper left corner patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the crop at the upper left corner position of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = 0\n",
    "    offset_width = 0\n",
    "\n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_corner_upper_right(image, target_height, target_width):\n",
    "    '''Crops the upper right corner patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the crop at the upper right corner position of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = 0\n",
    "    offset_width = (width - target_width)\n",
    "\n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_corner_lower_right(image, target_height, target_width):\n",
    "    '''Crops the lower right corner patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the crop at the lower right corner position of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = (height - target_height)\n",
    "    offset_width = (width - target_width)\n",
    "\n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_corner_lower_left(image, target_height, target_width):\n",
    "    '''Crops the lower left corner patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing the crop at the lower left corner position of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = (height - target_height)\n",
    "    offset_width = 0\n",
    "\n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_random(image, target_height, target_width):\n",
    "    '''Crops a random patch of an image\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        target_height (int): Height of the crop\n",
    "        target_width (int): Width of the crop\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: 3-D tensor containing a crop at a random position of the image\n",
    "    '''\n",
    "    \n",
    "    shape = tf.shape(input=image)\n",
    "    height, width = shape[0], shape[1]\n",
    "\n",
    "    offset_height = tf.random.uniform((1,), maxval=(height - target_height), dtype=tf.dtypes.int32)[0]\n",
    "    offset_width = tf.random.uniform((1,), maxval=(width - target_width), dtype=tf.dtypes.int32)[0]\n",
    "    \n",
    "    return crop_image(\n",
    "        image,\n",
    "        offset_height, offset_width,\n",
    "        target_height, target_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_horizontally(image):\n",
    "    '''Simple wrapper for `tf.image.flip_up_down`\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        \n",
    "    Returns:\n",
    "        flipped_image: 3-D tensor containing the horizontally flipped image\n",
    "    '''\n",
    "    \n",
    "    return tf.image.flip_up_down(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_vertically(image):\n",
    "    '''Simple wrapper for `tf.image.flip_left_right`\n",
    "    \n",
    "    Args:\n",
    "        image: 3-D image `Tensor`\n",
    "        \n",
    "    Returns:\n",
    "        flipped_image: 3-D tensor containing the vertically flipped image\n",
    "    '''\n",
    "    \n",
    "    return tf.image.flip_left_right(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_contrast_correction(feature_mat, scale=1, bias=0, eps=1e-3, rowvar=True):\n",
    "    '''Global contrast correction for a given feature matrix\n",
    "    \n",
    "    Performs global contrast correction for a given feature matrix (i.e. bias correction and\n",
    "    normalization resp. rescaling of the standard deviation)\n",
    "    \n",
    "    Args:\n",
    "        feature_mat (float): 2-D tensor containing the features to be contrast corrected\n",
    "        scale (int, optional): Rescaling factor for the standard deviation after normalizing; the\n",
    "            resulting image will thus have a standard deviation of `scale`\n",
    "        bias (int, optional): Regularization term to bias the standard deviation prior to normalizing\n",
    "        eps (float, optional): Cutoff constant to prevent division by zero; standard deviations below\n",
    "            this value are automatically set to `cutoff`\n",
    "        rowvar (bool, optional): Numpy-style indicator, whether the features are in the rows or in the\n",
    "            coloumns of the matrix (with the respective other dimension representing the observations);\n",
    "            True (default) means the features are in the rows, False implies the features are in the\n",
    "            coloumns\n",
    "            \n",
    "    Returns:\n",
    "        contrast_corrected_feature_mat: 2-D tensor containing the contrast corrected feature matrix\n",
    "            with feature mean zero and standard deviation `scale`\n",
    "    '''\n",
    "    \n",
    "    if not rowvar:\n",
    "        feature_mat = tf.transpose(feature_mat)\n",
    "        \n",
    "    # Offset / bias correction\n",
    "    feature_mat = feature_mat - tf.expand_dims(tf.reduce_mean(feature_mat, axis=1), axis=-1)\n",
    "    \n",
    "    # Standard deviation normalization and rescaling\n",
    "    feature_mat = scale * feature_mat / tf.math.maximum(eps, tf.math.sqrt(bias + tf.expand_dims(tf.reduce_mean(feature_mat ** 2, axis=1), axis=-1)))\n",
    "    \n",
    "    if not rowvar:\n",
    "        feature_mat = tf.transpose(feature_mat)\n",
    "    \n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorrelation of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zca_whitening(feature_mat, eps=1e-3, rowvar=True):\n",
    "    '''Performs ZCA whitening for a given feature matrix\n",
    "    \n",
    "    Performs ZCA whitening (aka \"Mahalanbobis transformation\") to de-correlate the data dimensions\n",
    "    of a given feature matrix following (Krizhevsky, 2009)\n",
    "    \n",
    "    Args:\n",
    "        feature_mat (float): 2-D tensor containing the features to be de-correlated\n",
    "        eps (float, optional): Cutoff constant to prevent division by zero; standard deviations below\n",
    "            this value are automatically set to `cutoff`\n",
    "        rowvar (bool, optional): Numpy-style indicator, whether the features are in the rows or in the\n",
    "            coloumns of the matrix (with the respective other dimension representing the observations);\n",
    "            True (default) means the features are in the rows, False implies the features are in the\n",
    "            coloumns\n",
    "            \n",
    "    Returns:\n",
    "        zca_whitened_feature_mat: 2-D tensor containing the ZCA whitened feature matrix with\n",
    "            de-correlated data dimensions\n",
    "    '''\n",
    "    \n",
    "    if not rowvar:\n",
    "        feature_mat = tf.transpose(feature_mat)\n",
    "    \n",
    "    # Calculate the covariance matrix (for zero-mean centered data: cov(X) = X * X.T / (n - 1)\n",
    "    cov = tf.linalg.matmul(feature_mat, feature_mat, transpose_b=True) / tf.cast(tf.shape(feature_mat)[1] - 1, tf.float32)\n",
    "            \n",
    "    # Singular value decomposition\n",
    "    s, u, _ = tf.linalg.svd(cov)\n",
    "    \n",
    "    # Calculate the ZCA whitening matrix (w = P * D ** (-1/2) * P with P an orthogonal and D a diagonal matrix constuting cov)\n",
    "    w = tf.linalg.matmul(u, tf.linalg.matmul(tf.linalg.diag(tf.math.maximum(eps, s) ** (-1/2)), u, transpose_b=True))\n",
    "        \n",
    "    # ZCA whitening of the feature matrix\n",
    "    feature_mat = tf.linalg.matmul(w, feature_mat)\n",
    "                                                                           \n",
    "    if not rowvar:\n",
    "        feature_mat = tf.transpose(feature_mat)                               \n",
    "    \n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General preprocessing applicable to all images (dataset level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_preprocessing_dataset_level(dataset):\n",
    "    '''Preprocessing steps applicable to all images\n",
    "    \n",
    "    Preprocessing steps applicable to all images; includes global contrast correction and ZCA whitening\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing image records in the format (`image`(float32), `label`)\n",
    "        \n",
    "    Returns:\n",
    "        processed_dataset: Preprocessed dataset of (`image`, `label`) tuples\n",
    "    '''\n",
    "    \n",
    "    # Get the number of entries in the dataset\n",
    "    dataset_length = 0\n",
    "    for _ in dataset:\n",
    "        dataset_length += 1\n",
    "    \n",
    "    # Transform the dataset to a 2-D tensor with each row containing one image in flattened form and\n",
    "    # a 1-D tensor containing the respective labels\n",
    "    (images, labels) = tf.data.experimental.get_single_element(dataset.batch(dataset_length))\n",
    "    image_dims = tf.shape(images)\n",
    "    images = tf.reshape(images, (image_dims[0], image_dims[1] * image_dims[2] * image_dims[3]))\n",
    "    \n",
    "    # Global contrast correction\n",
    "    images = global_contrast_correction(images, rowvar=False)\n",
    "    \n",
    "    # ZCA whitening\n",
    "    images = zca_whitening(images, rowvar=False)\n",
    "    \n",
    "    # Reconstruction of the original dataset structure\n",
    "    images = tf.reshape(images, (image_dims[0], image_dims[1], image_dims[2], image_dims[3]))\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General preprocessing applicable to all images (record level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_preprocessing_record_level(record, flip=False):\n",
    "    '''Preprocessing steps applicable to all images\n",
    "    \n",
    "    Preprocessing steps applicable to all images; includes horizontal flipping\n",
    "    \n",
    "    Args:\n",
    "        record: (`image`, `label`) tuple\n",
    "        flip (optional): Indicator whether to flip the image\n",
    "        \n",
    "    Returns:\n",
    "        processed_image: 3-D tensor containing the processed image\n",
    "        label: Label belonging to `processed_image`\n",
    "    '''\n",
    "    \n",
    "    image = record[0]\n",
    "    label = record[1]\n",
    "            \n",
    "    if flip:\n",
    "        processed_image = flip_horizontally(image)\n",
    "    else:\n",
    "        processed_image = image\n",
    "    \n",
    "    return (processed_image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing dependent on whether the images are used for training or evaluation (record level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocessing_record_level(record):\n",
    "    '''Preprocessing steps applicable only to training images\n",
    "    \n",
    "    Preprocessing steps applicable only to training images; includes cropping of random image patches\n",
    "    \n",
    "    Args:\n",
    "        record: (`image`, `label`) tuple\n",
    "        \n",
    "    Returns:\n",
    "        processed_image: 3-D tensor containing the processed image\n",
    "        label: Label belonging to `processed_image`\n",
    "    '''\n",
    "            \n",
    "    image = record[0]\n",
    "    label = record[1]\n",
    "    \n",
    "    processed_image = crop_random(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    \n",
    "    return (processed_image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocessing_record_level(record, crop_mode):\n",
    "    '''Preprocessing steps applicable only to test images\n",
    "    \n",
    "    Preprocessing steps applicable only to training images; includes cropping of center and corner\n",
    "    image patches\n",
    "    \n",
    "    Args:\n",
    "        record: (`image`, `label`) tuple\n",
    "        crop_mode (int): Indicates whether to crop at the center or the corners of the image ([0:3]\n",
    "            corners (clockwise starting at the upper left corner) and [4] image center)\n",
    "            \n",
    "    Returns:\n",
    "        processed_image: 3-D tensor containing the processed image\n",
    "        label: Label belonging to `processed_image`\n",
    "    '''\n",
    "    \n",
    "    image = record[0]\n",
    "    label = record[1]\n",
    "    \n",
    "    if crop_mode == 0:\n",
    "        processed_image = crop_corner_upper_left(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    elif crop_mode == 1:\n",
    "        processed_image = crop_corner_upper_right(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    elif crop_mode == 2:\n",
    "        processed_image = crop_corner_lower_right(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    elif crop_mode == 3:\n",
    "        processed_image = crop_corner_lower_left(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    else:\n",
    "        processed_image = crop_central(image, CROP_SIZE_H, CROP_SIZE_W)\n",
    "    \n",
    "    return (processed_image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a raw dataset, construct an iterator over the preprocessed and augmented records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_augment(dataset,\n",
    "                        batch_size,\n",
    "                        synset_level=CIFAR100_FINE_LABEL_LEVEL,\n",
    "                        hypernym=None,\n",
    "                        is_train=False,\n",
    "                        num_rnd_crops=5,\n",
    "                        shuffle_buffer_size=1000,\n",
    "                        num_epochs=1):\n",
    "    '''Given raw dataset, construct an iterator over the preprocessed and augmented records\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing raw records\n",
    "        batch_size: Number of samples per batch\n",
    "        synset_level (optional): Specifier of the synset level (i.e. fineness of label resolution)\n",
    "            to use (defaults to the finest possible resolution for the CIFAR100 dataset)\n",
    "        hypernym (optional): Identifier of the parental (hypernym) category whose subordinate (hyponym)\n",
    "            labels are to be included in the dataset (only evaluated below the root level, i.e. if\n",
    "            `synset_level` is greater than `0`; value `None` (default) indicates that all categories\n",
    "            are to be included; all records with other labels are discarded)\n",
    "        is_train (optional): Indicator whether the input is for training\n",
    "        num_rnd_crops (optional): Number of random crops perform per image (each crop is subsequently\n",
    "            flipped vertically as well, resulting in a total augmentation of 2 * `num_rnd_crops` per\n",
    "            image; only evaluated if `is_train`is set to True)\n",
    "        shuffle_buffer_size (optional): Size of the buffer to use when shuffling records (only used\n",
    "            if `is_train` is set to True)\n",
    "        num_epochs (optional): Number of epochs to repeat the dataset (only used when `is_train` is\n",
    "            set to True)\n",
    "            \n",
    "    Returns:\n",
    "        dataset: Dataset of (`image`, `label`) pairs ready for iteration\n",
    "    '''\n",
    "    \n",
    "    # Cut off invalid values for `synset_level` (has to be either `0` or `1` as the CIFAR100 dataset\n",
    "    # only has two levels of label granularity)\n",
    "    synset_level = max(0, min(CIFAR100_FINE_LABEL_LEVEL, synset_level))\n",
    "    \n",
    "    # Get the maximum label (called 'label depth' in Tensorflow) for one-hot encoding (cf. below)\n",
    "    label_depth = len(CIFAR100_SYNSET_MAP.get_all_synsets_of_level(synset_level))\n",
    "\n",
    "    # Convert each record to the form (image, label)\n",
    "    dataset = dataset.map(\n",
    "        lambda record: (record[CIFAR100_IMG_KEY],\n",
    "                        tf_cifar100_resolve_hypernym_encoded(record[CIFAR100_FINE_LABEL_KEY], synset_level)),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if (synset_level != 0) and (hypernym is not None):\n",
    "        # Check, whether `hypernym` is compatible with `synset_level`\n",
    "        if (CIFAR100_SYNSET_MAP.synset_level(hypernym) == synset_level - 1):\n",
    "            hyponyms = CIFAR100_SYNSET_MAP.hyponyms(hypernym)\n",
    "\n",
    "            if hyponyms:\n",
    "                # Apply filter w/ regards to the parental (coarse) category (i.e. filter out all images\n",
    "                # that don't belong to a given parent category)\n",
    "                dataset = dataset.filter(\n",
    "                    lambda _, label: tf.math.reduce_any(\n",
    "                        [tf.math.equal(label, cifar100_encode(hyponym)[0]) for hyponym in hyponyms]))\n",
    "    \n",
    "    # One-hot encode the remaining entries and convert the images to float32 to make subsequent\n",
    "    # calculations go smoothly\n",
    "    dataset = dataset.map(\n",
    "        lambda image, label: (tf.cast(image, tf.dtypes.float32), tf.one_hot(label, label_depth)),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # General (dataset level) preprocessing as described in (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky, 2009)\n",
    "    dataset = general_preprocessing_dataset_level(dataset)\n",
    "        \n",
    "    # Cache the dataset to prevent the subsequent record level preprocessing steps to re-run the\n",
    "    # dataset level preprocessing for each record (only applicable if we're not filtering; in that\n",
    "    # case we're only processing a small fraction of the dataset at once so we don't necessarily\n",
    "    # need the caching anyway)\n",
    "    if (synset_level == 0) or (hypernym is None):\n",
    "        dataset = dataset.cache()\n",
    "\n",
    "    # Parse the raw records into images and labels and augment the dataset as described in\n",
    "    # (Krizhevsky et al., 2012)\n",
    "    \n",
    "    # Repeat each image twice (once per flip)\n",
    "    dataset = dataset.enumerate().interleave(\n",
    "        lambda _, record: tf.data.Dataset.from_tensors(record).repeat(2),\n",
    "        cycle_length=1,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # General (record level) preprocessing applicable to all images\n",
    "    dataset = dataset.enumerate().map(\n",
    "        lambda idx, record: general_preprocessing_record_level(record, flip=tf.dtypes.cast(idx % 2, tf.dtypes.bool)),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "    # Mode dependent augmentation\n",
    "    if is_train:\n",
    "        # Repeat each image `num_rnd_crops` times (once per crop)\n",
    "        dataset = dataset.enumerate().interleave(\n",
    "            lambda _, record: tf.data.Dataset.from_tensors(record).repeat(num_rnd_crops),\n",
    "            cycle_length=1,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "        dataset = dataset.enumerate().map(\n",
    "            lambda idx, record: train_preprocessing_record_level(record),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    else:\n",
    "        # Repeat each image five times (once per crop)\n",
    "        dataset = dataset.enumerate().interleave(\n",
    "            lambda _, record: tf.data.Dataset.from_tensors(record).repeat(5),\n",
    "            cycle_length=1,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        dataset = dataset.enumerate().map(\n",
    "            lambda idx, record: test_preprocessing_record_level(record, crop_mode=idx%5),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if is_train:\n",
    "        # Shuffle records before repeating to respect epoch boundaries\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        # Repeats the dataset for the number of epochs to train\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "    \n",
    "    # Batching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Operations between the final prefetch and the get_next call to the iterator will happen\n",
    "    # synchronously during run time. Manual prefetching at this point backgrounds all of the above\n",
    "    # processing work and helps keep it out of the critical training path.\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the performance of the preprocessing pipeline\n",
    "\n",
    "# Number of samples to use for the benchmark (higher numbers result in a lower bias from onetime\n",
    "# operations but increase the computation time)\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "cifar100_train = process_and_augment(cifar100_train_raw,\n",
    "                                     batch_size=1,\n",
    "                                     synset_level=CIFAR100_FINE_LABEL_LEVEL,\n",
    "                                     hypernym='aquatic_mammals',\n",
    "                                     is_train=True,\n",
    "                                     num_rnd_crops=5,\n",
    "                                     shuffle_buffer_size=1000,\n",
    "                                     num_epochs=1)\n",
    "\n",
    "start_time_raw = time.perf_counter()\n",
    "for record in cifar100_train_raw.take(NUM_SAMPLES):\n",
    "    continue\n",
    "end_time_raw = time.perf_counter()\n",
    "\n",
    "mean_time_raw = (end_time_raw - start_time_raw) / NUM_SAMPLES\n",
    "print('Mean time per sample (raw): {} s'.format(mean_time_raw))\n",
    "\n",
    "start_time_processed = time.perf_counter()\n",
    "for record in cifar100_train.take(NUM_SAMPLES):\n",
    "    continue\n",
    "end_time_processed = time.perf_counter()\n",
    "\n",
    "mean_time_processed = (end_time_processed - start_time_processed) / NUM_SAMPLES\n",
    "print('Mean time per sample (preprocessed): {} s'.format(mean_time_processed))\n",
    "\n",
    "mean_time_diff = mean_time_processed - mean_time_raw\n",
    "print('Time difference per sample: {} s'.format(mean_time_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Composed Network (CompNet)\n",
    "<a id='compnet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examine the performance of a composed model on the CIFAR100 dataset. \n",
    "\n",
    "The structure of the model is based on the inherent structure of the CIFAR100 dataset, i.e. two interconnected \"layers\" of classifiers where the predictions of the first layer are used to route the inputs to the corresponding sub-modules of the second, specialized layer. For simplicity, we keep the basic architecture and hyperparameters for each sub-module the same (i.e. we don't perform inidividual hyperparameter tuning) and limit the amount of data each module is trained upon to the minimum number of examples one of the (coarse) categories is comprised of. Furthermore, for comparability with results reported in literature as well as our own benchmark (cf. below), we employ MaxOut units (Goodfellow et al., 2013) at the convolutional as well as at the fully connected layers.\n",
    "\n",
    "It should be noted at this point, that, even though there are several promising approaches to be found in literature regarding possible improvements of the resilience of hierarchically composed models (cf. e.g. (Fergus et al., 2010), (Deng et al., 2014) or (Roy et al., 2019)), we deliberately avoided incorporating these ideas into our work as the goal of this project is to examine the question whether modularisation of networks in general is a feasible approach and we want to keep our results as unbiased as possible in this regard (i.e. not \"unfairly\" improving our approach with regard to the benchmark). Nevertheless, it should also be noted that in doing so, we leave lots of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "<a id='compnet_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='compnet_model_maxout'></a>\n",
    "In a convolutional network, MaxOut feature maps can be constructed by concatenating the following elements (Goodfellow et. al, 2013):\n",
    "    \n",
    "    Input: (batch_size, image_width, image_heigth, num_channels)\n",
    "        tf.keras.Layers.conv2D w/ activaton=None\n",
    "    Intermediate output: (batch_size, image_width, image_heigth, num_filters)\n",
    "        tf.keras.layers.Reshape w/ dimensions (image_width - filter_width + 1, image_heigth - filter_height + 1, num_filters, 1)\n",
    "    Intermediate output: (batch_size, image_width, image_heigth, num_filters, 1)\n",
    "        tf.keras.Layers.MaxPool3D over the affine feature maps\n",
    "    Intermediate output: (batch_size, image_width, image_heigth, num_filters / pool_size, 1)\n",
    "        tf.keras.layers.Reshape w/ dimensions (image_width - filter_width + 1, image_heigth - filter_height + 1, num_filters / pool_size)\n",
    "    Final output: (batch_size, image_width, image_heigth, num_filters / pool_size)\n",
    "\n",
    "On the topic of native API versus custom layers: Usually, one would argue, that - considering the \"hack\" that is used in the form of reshaping to enable cross-channel pooling - it might be a good idea to implement a custom MaxOut layer instead to increase the readability of the code and remove unnecessary operations from the model (i.e. increase the \"execution smoothness\" of the model. Even though, as goal of this project is the scientific evaluation of the effect of our proposed method, we against this (usually good) practice to keep our \"side-channel error vector\" as small as possible (in this specific case: original errors induced by faulty programming) to reduce the risk of accidental result distortion. Additionally, we believe that by limiting ourselves to the utilization of standard functions provided by the native API, we increase the reproducability of our experiments even across framework boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We imply a combination of Dropout and MaxNorm for weight regularization as is reported\n",
    "# in (Srivastava et al., 2014) to be most effective. Furthermore, we also apply Dropout to the\n",
    "# convolutional layers to facilitate the discovery of informative features as described in\n",
    "# (Park et al., 2016).\n",
    "\n",
    "# Input specification\n",
    "inputs = tf.keras.Input(shape=(28, 28, 3))\n",
    "\n",
    "# First convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=64,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           activation=None,\n",
    "                           use_bias=True,\n",
    "                           kernel_constraint='max_norm',\n",
    "                           bias_constraint='max_norm')(inputs)\n",
    "x = tf.keras.layers.Reshape((28, 28, 64, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                              strides=(1, 1, 4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((28, 28, 16))(x)\n",
    "\n",
    "# First MaxPool layer\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              padding='same')(x)\n",
    "\n",
    "# First Dropout layer\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "# Second convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=128,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           activation=None,\n",
    "                           use_bias=True,\n",
    "                           kernel_constraint='max_norm',\n",
    "                           bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((14, 14, 128, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                              strides=(1, 1, 4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((14, 14, 32))(x)\n",
    "\n",
    "# Second MaxPool layer\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              padding='same')(x)\n",
    "\n",
    "# First Second layer\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "# Third convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=1,\n",
    "                        padding='same',\n",
    "                        activation=None,\n",
    "                        use_bias=True,\n",
    "                        kernel_constraint='max_norm',\n",
    "                        bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((7, 7, 256, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                            strides=(1, 1, 4),\n",
    "                            padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((7, 7, 64))(x)\n",
    "\n",
    "# Transition between convolutional and fully connected layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Third Dropout layer\n",
    "x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Fully connected MaxOut layer\n",
    "x = tf.keras.layers.Dense(units=128,\n",
    "                          activation=None,\n",
    "                          use_bias=True,\n",
    "                          kernel_constraint='max_norm',\n",
    "                          bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((128, 1))(x)\n",
    "x = x = tf.keras.layers.MaxPool1D(pool_size=(4),\n",
    "                              strides=(4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Fourth Dropout layer\n",
    "x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Softmax output layer\n",
    "outputs = tf.keras.layers.Dense(units=CIFAR100_NUM_COARSE_LABELS,\n",
    "                                activation='softmax',\n",
    "                                use_bias=True,\n",
    "                                kernel_constraint='max_norm',\n",
    "                                bias_constraint='max_norm')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "coarse_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Categorical Cross Entropy as loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Define metrics to watch during training\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "           tf.keras.metrics.CategoricalCrossentropy(),\n",
    "           tf.keras.metrics.AUC()]\n",
    "\n",
    "# Use Adam (Kingma et al., 2017) as optimizer during training\n",
    "# Annotation: We don't set `learning_rate` here as this is automatically handled by the\n",
    "# LearningRateScheduler (cf. callback section below).\n",
    "optimizer = tf.keras.optimizers.Adam(beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: Restore model from checkpoint\n",
    "# coarse_model = tf.keras.models.load_model(CKPT_DIR + 'compnet_coarse')\n",
    "# coarse_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got as many subordinate specialist modules as there are coarse categories (cf. https://www.cs.toronto.edu/~kriz/cifar.html for a comprehensive list). The downstream models are grouped in an array in order of their respective coarse label (i.e. 0..n where n is the number of coarse categories) to facilitate routing between the two layers by enabling the direct utilization of the prediction of the first layer for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models = []\n",
    "\n",
    "for _ in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    # Note: We imply a combination of Dropout and MaxNorm for weight regularization as is reported\n",
    "    # in (Srivastava et al., 2014) to be most effective. Furthermore, we also apply Dropout to the\n",
    "    # convolutional layers to facilitate the discovery of informative features as described in\n",
    "    # (Park et al., 2016).\n",
    "\n",
    "    # Input specification\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 3))\n",
    "\n",
    "    # First convolutional MaxOut layer\n",
    "    x = tf.keras.layers.Conv2D(filters=64,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation=None,\n",
    "                               use_bias=True,\n",
    "                               kernel_constraint='max_norm',\n",
    "                               bias_constraint='max_norm')(inputs)\n",
    "    x = tf.keras.layers.Reshape((28, 28, 64, 1))(x)\n",
    "    x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                                  strides=(1, 1, 4),\n",
    "                                  padding='same')(x)\n",
    "    x = tf.keras.layers.Reshape((28, 28, 16))(x)\n",
    "\n",
    "    # First MaxPool layer\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                  strides=(2, 2),\n",
    "                                  padding='same')(x)\n",
    "\n",
    "    # First Dropout layer\n",
    "    x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "    # Second convolutional MaxOut layer\n",
    "    x = tf.keras.layers.Conv2D(filters=128,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation=None,\n",
    "                               use_bias=True,\n",
    "                               kernel_constraint='max_norm',\n",
    "                               bias_constraint='max_norm')(x)\n",
    "    x = tf.keras.layers.Reshape((14, 14, 128, 1))(x)\n",
    "    x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                                  strides=(1, 1, 4),\n",
    "                                  padding='same')(x)\n",
    "    x = tf.keras.layers.Reshape((14, 14, 32))(x)\n",
    "\n",
    "    # Second MaxPool layer\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                  strides=(2, 2),\n",
    "                                  padding='same')(x)\n",
    "\n",
    "    # Transition between convolutional and fully connected layers\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Second Dropout layer\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "    # Fully connected MaxOut layer\n",
    "    x = tf.keras.layers.Dense(units=128,\n",
    "                              activation=None,\n",
    "                              use_bias=True,\n",
    "                              kernel_constraint='max_norm',\n",
    "                              bias_constraint='max_norm')(x)\n",
    "    x = tf.keras.layers.Reshape((128, 1))(x)\n",
    "    x = x = tf.keras.layers.MaxPool1D(pool_size=(4),\n",
    "                                  strides=(4),\n",
    "                                  padding='same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Third Dropout layer\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "    # Softmax output layer\n",
    "    outputs = tf.keras.layers.Dense(units=CIFAR100_NUM_FINE_LABELS,\n",
    "                                    activation='softmax',\n",
    "                                    use_bias=True,\n",
    "                                    kernel_constraint='max_norm',\n",
    "                                    bias_constraint='max_norm')(x)\n",
    "    \n",
    "    # Build the current model and append it to the list of sub-modules\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    fine_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    # Use Categorical Cross Entropy as loss function\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Define metrics to watch during training\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "               tf.keras.metrics.CategoricalCrossentropy(),\n",
    "               tf.keras.metrics.AUC()]\n",
    "\n",
    "    # Use Adam (Kingma et al., 2017) as optimizer during training\n",
    "    # Annotation: We don't set `learning_rate` here as this is automatically handled by the\n",
    "    # LearningRateScheduler (cf. callback section below).\n",
    "    optimizer = tf.keras.optimizers.Adam(beta_1=0.9,\n",
    "                                         beta_2=0.999,\n",
    "                                         epsilon=1e-07)\n",
    "    \n",
    "    # Compile the respective sub-module\n",
    "    fine_models[idx].compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: Restore models from checkpoint\n",
    "# fine_models = []\n",
    "# for idx in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "#     model = tf.keras.models.load_model(CKPT_DIR + 'compnet_fine_' + str(idx))\n",
    "#     fine_models.append(model)\n",
    "#\n",
    "# fine_models[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id='compnet_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_train_coarse = process_and_augment(cifar100_train_raw, batch_size=32, synset_level=CIFAR100_COARSE_LABEL_LEVEL, is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1)\n",
    "cifar100_val_coarse = process_and_augment(cifar100_val_raw, batch_size=32, synset_level=CIFAR100_COARSE_LABEL_LEVEL, is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping: Stop training early if no significant improvement in the monitored quantity is\n",
    "#     observed for at least `patience` epochs\n",
    "# LearningRateScheduler: Dynamically adapt the learning rate depending on the training epoch to\n",
    "#     facilitate accelerated learning during the first few epochs\n",
    "# ModelCheckpoint: Save the model after each epoch (if `save_best_only` is set to True, only keep\n",
    "#     the best model with regard to the monitored quantity)\n",
    "# TensorBoard: Enable TensorBoard visualization\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=0.01,\n",
    "                                              patience=3),\n",
    "             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-03 if epoch < 3 else 1e-04),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_DIR + 'compnet_coarse',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=False,\n",
    "                                                save_best_only=True),\n",
    "             tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
    "                                            histogram_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.fit(x=cifar100_train_coarse,\n",
    "                 epochs=100,\n",
    "                 verbose=True,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_data=cifar100_val_coarse,\n",
    "                 shuffle=True,\n",
    "                 validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_train_fine = [process_and_augment(cifar100_train_raw, batch_size=32, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=cifar100_decode(category, CIFAR100_COARSE_LABEL_LEVEL), is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1) for category in range(CIFAR100_NUM_COARSE_LABELS)] \n",
    "cifar100_val_fine = [process_and_augment(cifar100_val_raw, batch_size=32, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=cifar100_decode(category, CIFAR100_COARSE_LABEL_LEVEL), is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1) for category in range(CIFAR100_NUM_COARSE_LABELS)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of data points in each training and validation set for the different sub-modules\n",
    "# and determine the minimum amount of examples among them (used as a restrictive factor during the\n",
    "# training procedure; cf. below)\n",
    "\n",
    "dataset_length_train = []\n",
    "dataset_length_val = []\n",
    "\n",
    "for dataset in cifar100_train_fine:\n",
    "    dataset_length = 0\n",
    "    for _ in dataset:\n",
    "        dataset_length += 1\n",
    "\n",
    "    dataset_length_train.append(dataset_length)\n",
    "\n",
    "min_dataset_length_train = min(dataset_length_train)\n",
    "        \n",
    "for dataset in cifar100_val_fine:\n",
    "    dataset_length = 0\n",
    "    for _ in dataset:\n",
    "        dataset_length += 1\n",
    "\n",
    "    dataset_length_val.append(dataset_length)\n",
    "\n",
    "min_dataset_length_val = min(dataset_length_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    # EarlyStopping: Stop training early if no significant improvement in the monitored quantity is\n",
    "    #     observed for at least `patience` epochs\n",
    "    # LearningRateScheduler: Dynamically adapt the learning rate depending on the training epoch to\n",
    "    #     facilitate accelerated learning during the first few epochs\n",
    "    # ModelCheckpoint: Save the model after each epoch (if `save_best_only` is set to True, only keep\n",
    "    #     the best model with regard to the monitored quantity)\n",
    "    # TensorBoard: Enable TensorBoard visualization\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=0.01,\n",
    "                                                  patience=3),\n",
    "                 tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-03 if epoch < 3 else 1e-04),\n",
    "                 tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_DIR + 'compnet_fine_' + str(idx),\n",
    "                                                    monitor='val_loss',\n",
    "                                                    verbose=False,\n",
    "                                                    save_best_only=True),\n",
    "                 tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
    "                                                histogram_freq=1)]\n",
    "    \n",
    "    fine_models[idx].fit(x=cifar100_train_fine[idx].take(min_dataset_length_train),\n",
    "                         epochs=100,\n",
    "                         verbose=True,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=cifar100_val_fine[idx].take(min_dataset_length_val),\n",
    "                         shuffle=True,\n",
    "                         validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "<a id='compnet_test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_test_coarse = process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_COARSE_LABEL_LEVEL, is_train=False)\n",
    "cifar100_test_fine = [process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=cifar100_decode(category, CIFAR100_COARSE_LABEL_LEVEL), is_train=False) for category in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "cifar100_test_composed = process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to watch during the evaluation of the model on the test data set\n",
    "\n",
    "# Use the same metrics as for the training\n",
    "# test_metrics = metrics\n",
    "\n",
    "# Use different metrics than during the training\n",
    "test_metrics = [tf.keras.metrics.CategoricalAccuracy(name='CategoricalAccuracy'),\n",
    "                tf.keras.metrics.CategoricalCrossentropy(name='CategoricalCrossentropy'),\n",
    "                tf.keras.metrics.AUC(name='AUC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all metrics before starting the evaluation\n",
    "for metric in test_metrics:\n",
    "    metric.reset_states()\n",
    "\n",
    "for (imgs, ground_truths) in cifar100_test_coarse:\n",
    "    # Generate predictions for each image in the current batch\n",
    "    batch_scores = coarse_model.predict(imgs)\n",
    "    \n",
    "    # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "    # version of the same base image, we can simply average the individual scores to get the final\n",
    "    # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky et al., 2012).\n",
    "    prediction = tf.math.reduce_mean(batch_scores, axis=0)\n",
    "    \n",
    "    # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "    # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "    for metric in test_metrics:\n",
    "        metric.update_state(ground_truths[0], prediction)\n",
    "\n",
    "print('Coarse Model')\n",
    "print()\n",
    "print('==================================================')\n",
    "print()\n",
    "print('Final results:')\n",
    "for metric in test_metrics:\n",
    "    print('{}: {}'.format(metric.name, metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    # Define metrics to watch during the evaluation of the model on the test data set\n",
    "\n",
    "    # Use the same metrics as for the training\n",
    "    # test_metrics = metrics\n",
    "\n",
    "    # Use different metrics than during the training\n",
    "    test_metrics = [tf.keras.metrics.CategoricalAccuracy(name='CategoricalAccuracy'),\n",
    "                    tf.keras.metrics.CategoricalCrossentropy(name='CategoricalCrossentropy'),\n",
    "                    tf.keras.metrics.AUC(name='AUC')]\n",
    "\n",
    "    # Reset all metrics before starting the evaluation\n",
    "    for metric in test_metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "    for (imgs, ground_truths) in cifar100_test_fine[idx]:\n",
    "        # Generate predictions for each image in the current batch\n",
    "        batch_scores = fine_models[idx].predict(imgs)\n",
    "\n",
    "        # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "        # version of the same base image, we can simply average the individual scores to get the final\n",
    "        # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "        # (Krizhevsky et al., 2012).\n",
    "        prediction = tf.math.reduce_mean(batch_scores, axis=0)\n",
    "\n",
    "        # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "        # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "        for metric in test_metrics:\n",
    "            metric.update_state(ground_truths[0], prediction)\n",
    "\n",
    "    print('Fine Model #{}'.format(cifar100_decode(idx, CIFAR100_COARSE_LABEL_LEVEL)))\n",
    "    print()\n",
    "    print('==================================================')\n",
    "    print()\n",
    "    print('Final results:')\n",
    "    for metric in test_metrics:\n",
    "        print('{}: {}'.format(metric.name, metric.result().numpy()))\n",
    "    print()\n",
    "    print('====================================================================================================')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to watch during the evaluation of the model on the test data set\n",
    "\n",
    "# Use the same metrics as for the training\n",
    "# test_metrics = metrics\n",
    "\n",
    "# Use different metrics than during the training\n",
    "test_metrics = [tf.keras.metrics.CategoricalAccuracy(name='CategoricalAccuracy'),\n",
    "                tf.keras.metrics.CategoricalCrossentropy(name='CategoricalCrossentropy'),\n",
    "                tf.keras.metrics.AUC(name='AUC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all metrics before starting the evaluation\n",
    "for metric in test_metrics:\n",
    "    metric.reset_states()\n",
    "    \n",
    "# Initialize additional custom metrics to watch during the evaluation\n",
    "\n",
    "# Overall label distribution (predicted and ground truth)\n",
    "dist_ground_truth = [0] * CIFAR100_NUM_FINE_LABELS\n",
    "dist_predicted = [0] * CIFAR100_NUM_FINE_LABELS\n",
    "\n",
    "# Predicted label distribution for each coarse category\n",
    "dist_predicted_coarse = [[0] * CIFAR100_NUM_COARSE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "\n",
    "# Predicted label distribution for each fine category\n",
    "dist_predicted_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_FINE_LABELS)]\n",
    "\n",
    "# Fine label distribution for each coarse category (predicted and ground truth)\n",
    "dist_ground_truth_coarse_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "dist_predicted_coarse_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "\n",
    "# Semantic distance between the predicted category and the ground truth in accordance to\n",
    "# (Fergus et al., 2010) \n",
    "semantic_distance = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (imgs, ground_truths) in cifar100_test_composed:\n",
    "    # Generate predictions for each image in the current batch from the coarse (parental)\n",
    "    batch_scores_coarse = coarse_model.predict(imgs)\n",
    "    \n",
    "    # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "    # version of the same base image, we can simply average the individual scores to get the final\n",
    "    # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky et al., 2012).\n",
    "    prediction_coarse = tf.math.reduce_mean(batch_scores_coarse, axis=0)\n",
    "    \n",
    "    # Route the current batch to the sub-module predcited by the coarse model and generate the\n",
    "    # predictions for the respective fine category labels\n",
    "    batch_scores_fine = fine_models[tf.math.argmax(prediction_coarse)].predict(imgs)\n",
    "    \n",
    "    # Average the scores for the individual images in the batch as described above\n",
    "    prediction_fine = tf.math.reduce_mean(batch_scores_fine, axis=0)\n",
    "    \n",
    "    # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "    # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "    for metric in test_metrics:\n",
    "        metric.update_state(ground_truths[0], prediction_fine)\n",
    "        \n",
    "    # Update custom metrics w/ the result for the current base image; cf. above concerning the\n",
    "    # ground truth for each batch\n",
    "    \n",
    "    ground_truth_fine = tf.math.argmax(ground_truths[0]).numpy()\n",
    "    ground_truth_fine_decoded = cifar100_decode(ground_truth_fine, CIFAR100_FINE_LABEL_LEVEL)\n",
    "    \n",
    "    ground_truth_coarse = cifar100_resolve_hypernym(ground_truth_fine, level=CIFAR100_COARSE_LABEL_LEVEL, encoded=True)\n",
    "    \n",
    "    prediction_fine = tf.math.argmax(prediction_fine).numpy()\n",
    "    prediction_fine_decoded = cifar100_decode(prediction_fine, CIFAR100_FINE_LABEL_LEVEL)\n",
    "    \n",
    "    prediction_coarse = tf.math.argmax(prediction_coarse).numpy()\n",
    "    \n",
    "    dist_ground_truth[ground_truth_fine] += 1\n",
    "    dist_predicted[prediction_fine] += 1\n",
    "\n",
    "    dist_predicted_coarse[ground_truth_coarse][prediction_coarse] += 1\n",
    "\n",
    "    dist_predicted_fine[ground_truth_fine][prediction_fine] += 1\n",
    "\n",
    "    dist_ground_truth_coarse_fine[ground_truth_coarse][ground_truth_fine] += 1\n",
    "    dist_predicted_coarse_fine[ground_truth_coarse][prediction_fine] += 1\n",
    "    \n",
    "    semantic_distance += CIFAR100_SYNSET_MAP.semantic_distance(\n",
    "        ground_truth_fine_decoded,\n",
    "        prediction_fine_decoded\n",
    "    )\n",
    "\n",
    "print('Composed Model')\n",
    "print()\n",
    "print('==================================================')\n",
    "print()\n",
    "print('Final results:')\n",
    "for metric in test_metrics:\n",
    "    print('{}: {}'.format(metric.name, metric.result().numpy()))\n",
    "print('Semantic distance: {}'.format(semantic_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_ground_truth.data',\n",
    "    tf.io.serialize_tensor(dist_ground_truth))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted.data',\n",
    "    tf.io.serialize_tensor(dist_predicted))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_coarse.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_coarse))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_fine.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_ground_truth_coarse_fine.data',\n",
    "    tf.io.serialize_tensor(dist_ground_truth_coarse_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_coarse_fine.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_coarse_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'semantic_distance.data',\n",
    "    tf.io.serialize_tensor(semantic_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "dist_ground_truth = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_ground_truth.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_coarse = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_coarse.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_ground_truth_coarse_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_ground_truth_coarse_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_coarse_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'dist_predicted_coarse_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "semantic_distance = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_COMPNET + 'semantic_distance.data'),\n",
    "        tf.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(21, 5.5))\n",
    "fig.suptitle('Predicted vs Ground Truth Distribution')\n",
    "\n",
    "titles = ['Predicted', 'Ground Truth']\n",
    "\n",
    "for idx, dist in enumerate([dist_predicted, dist_ground_truth]):\n",
    "    # Normalize the distribution\n",
    "    dist = list(map(lambda entry: entry / sum(dist) * 100, dist)) \n",
    "                \n",
    "    # Plot the distribution\n",
    "    ax[idx].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[idx].set_title(titles[idx])\n",
    "    ax[idx].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "    ax[idx].set_xlim([0.5, 100.5])\n",
    "    ax[idx].set_ylim([0, 3.3])\n",
    "    ax[idx].set_xlabel('Label')\n",
    "    ax[idx].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_compnet_dist_pred_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 2, figsize=(21, 65.5))\n",
    "fig.suptitle('Predicted Coarse Distribution per Coarse Ground Truth', y=0.88885)\n",
    "\n",
    "for label in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    row = int(label / 2)\n",
    "    col = label % 2\n",
    "\n",
    "    # Normalize the distribution\n",
    "    dist = list(map(\n",
    "        lambda entry: entry / sum(dist_predicted_coarse[label]) * 100, dist_predicted_coarse[label])) \n",
    "\n",
    "    # Plot the distribution\n",
    "    ax[row][col].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[row][col].set_title(\n",
    "        '{}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL))\n",
    "    )\n",
    "    ax[row][col].set_xticks(range(0, CIFAR100_NUM_COARSE_LABELS, 2))\n",
    "    ax[row][col].set_xlim([0.5, 20.5])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "    ax[row][col].set_xlabel('Label')\n",
    "    ax[row][col].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_compnet_dist_pred_coarse_per_coarse_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(50, 2, figsize=(21, 330.5))\n",
    "fig.suptitle('Predicted Fine Distribution per Fine Ground Truth', y=0.88175)\n",
    "\n",
    "for label in range(CIFAR100_NUM_FINE_LABELS):\n",
    "    row = int(label / 2)\n",
    "    col = int(label % 2)\n",
    "    \n",
    "    # Normalize the distribution\n",
    "    dist = list(map(\n",
    "        lambda entry: entry / sum(dist_predicted_fine[label]) * 100, dist_predicted_fine[label])) \n",
    "\n",
    "    # Plot the distribution\n",
    "    ax[row][col].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[row][col].set_title(\n",
    "        '{}'.format(cifar100_decode(label, CIFAR100_FINE_LABEL_LEVEL))\n",
    "    )\n",
    "    ax[row][col].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "    ax[row][col].set_xlim([0.5, 100.5])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "    ax[row][col].set_xlabel('Label')\n",
    "    ax[row][col].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_compnet_dist_pred_fine_per_fine_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, 2, figsize=(21, 130.5))\n",
    "fig.suptitle('Predicted vs Ground Truth Fine Distribution per Coarse Ground Truth', y=0.88425)\n",
    "\n",
    "for label in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    titles = ['Predicted for {}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL)),\n",
    "          'Ground Truth for {}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL))]\n",
    "\n",
    "    for idx, dist in enumerate([dist_predicted_coarse_fine, dist_ground_truth_coarse_fine]):\n",
    "        # Normalize the distribution\n",
    "        dist = list(map(lambda entry: entry / sum(dist[label]) * 100, dist[label])) \n",
    "                    \n",
    "        # Plot the distribution\n",
    "        ax[label][idx].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "        ax[label][idx].set_title(titles[idx])\n",
    "        ax[label][idx].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "        ax[label][idx].set_xlim([0.5, 100.5])\n",
    "        ax[label][idx].set_ylim([0, 50])\n",
    "        ax[label][idx].set_xlabel('Label')\n",
    "        ax[label][idx].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_compnet_dist_pred_groundTruth_fine_per_coarse_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive uncertainty under dataset shift\n",
    "<a id='compnet_predictive_uncertainty'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examine how the composed model behaves with respect to predictive uncertainty under dataset shift. Methodologically, we follow the basic approach described by (Ovadia et. al, 2019) and, in doing so, employ for the shifted in-distribution data the corrupted CIFAR100-C dataset (Hendrycks et al., 2019) resp. the CIFAR10 dataset as out-of-distribution (OOD) reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR100-C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specific configuration\n",
    "\n",
    "# Storage location of the CIFAR100-C dataset\n",
    "CIFAR100_C_DATA_DIR = TFDS_DATA_DIR + 'cifar100/corrupted/'\n",
    "\n",
    "# List of corruption types to include in the dataset on load\n",
    "# Annotation: For a comprehensive list of all corruption types, cf. https://github.com/hendrycks/robustness).\n",
    "CIFAR100_C_CORRUPTIONS = [\n",
    "    'defocus_blur',\n",
    "    'gaussian_blur',\n",
    "    'glass_blur',\n",
    "    'motion_blur',\n",
    "    'zoom_blur',\n",
    "    'gaussian_noise',\n",
    "    'impulse_noise',\n",
    "    'shot_noise',\n",
    "    'speckle_noise',\n",
    "    'fog',\n",
    "    'frost',\n",
    "    'snow',\n",
    "    'brightness',\n",
    "    'contrast',\n",
    "    'saturate',\n",
    "    'elastic_transform',\n",
    "    'jpeg_compression',\n",
    "    'pixelate',\n",
    "    'spatter'\n",
    "]\n",
    "\n",
    "# Indicator whether to include the uncorrupted dataset for reference in addition to the above\n",
    "# corruptions during the subsequent evaluation\n",
    "CIFAR100_C_INCLUDE_UNCORRUPTED = True\n",
    "\n",
    "# Number of severity grades and respectively associated samples per corruption\n",
    "CIFAR100_C_NUM_SEVERITY_GRADES = (5 * (1 if CIFAR100_C_CORRUPTIONS else 0) + CIFAR100_C_INCLUDE_UNCORRUPTED)\n",
    "CIFAR100_C_NUM_SAMPLES_PER_SEVERITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR100-C dataset\n",
    "\n",
    "# Template of the structure of the individual records to apply when parsing the serialized dataset\n",
    "record_template = {\n",
    "    CIFAR100_IMG_KEY: tf.io.FixedLenFeature([], tf.dtypes.string, default_value=''),\n",
    "    CIFAR100_FINE_LABEL_KEY: tf.io.FixedLenFeature([], tf.dtypes.int64, default_value=-1)\n",
    "}\n",
    "\n",
    "# Load the dataset including all corruptions specified by `CIFAR100_C_CORRUPTIONS`\n",
    "cifar100_c_test_raw = tf.data.TFRecordDataset([CIFAR100_C_DATA_DIR + corruption + '.tfrecord' for corruption in CIFAR100_C_CORRUPTIONS])\n",
    "\n",
    "# Parse the serialized `tf.train.Example` protos\n",
    "cifar100_c_test_raw = cifar100_c_test_raw.map(\n",
    "    lambda record: tf.io.parse_single_example(record, record_template),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# JPEG-decode the images\n",
    "cifar100_c_test_raw = cifar100_c_test_raw.map(\n",
    "    lambda record: {CIFAR100_IMG_KEY: tf.io.decode_jpeg(record[CIFAR100_IMG_KEY]),\n",
    "                    CIFAR100_FINE_LABEL_KEY: record[CIFAR100_FINE_LABEL_KEY]},\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specific configuration\n",
    "\n",
    "# Number of samples in the test dataset\n",
    "CIFAR10_NUM_TEST_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test_raw, cifar10_info = tfds.load('cifar10',\n",
    "                                           split='test',\n",
    "                                           data_dir=TFDS_DATA_DIR,\n",
    "                                           download_and_prepare_kwargs={'download_dir': TFDS_DOWNLOAD_DIR},\n",
    "                                           with_info=True)\n",
    "print(cifar10_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributional shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_c_test = [None] * CIFAR100_C_NUM_SEVERITY_GRADES\n",
    "\n",
    "if CIFAR100_C_INCLUDE_UNCORRUPTED:\n",
    "    # Include the uncorrupted test dataset for reference\n",
    "    cifar100_c_test[0] = process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)\n",
    "\n",
    "# Construct one test dataset per severity grade\n",
    "for corruption in range(len(CIFAR100_C_CORRUPTIONS)):\n",
    "    print('Corruption: {}'.format(corruption))\n",
    "\n",
    "    for severity in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "        print('Severity: {}'.format(severity))\n",
    "\n",
    "        # As the corruption files are read in consecutively and each file contains\n",
    "        # `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` * `CIFAR100_C_NUM_SEVERITY_GRADES` records with the\n",
    "        # first `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` samples being of the lowest severity grade\n",
    "        # followed by `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` samples of the secont severity grade, etc.,\n",
    "        # we can construct a dataset containing only samples of corruption `corruption` and severity\n",
    "        # grade `severity` as follows:\n",
    "        dataset = cifar100_c_test_raw.enumerate().filter(\n",
    "            lambda idx, _: int(idx / CIFAR100_C_NUM_SAMPLES_PER_SEVERITY) == corruption * (CIFAR100_C_NUM_SEVERITY_GRADES - CIFAR100_C_INCLUDE_UNCORRUPTED) + severity)\n",
    "        \n",
    "        # Remove the index residue from the previous filtering operation\n",
    "        dataset = dataset.map(\n",
    "            lambda _, record: record,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        dataset = process_and_augment(dataset, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)\n",
    "        \n",
    "        idx = severity + CIFAR100_C_INCLUDE_UNCORRUPTED\n",
    "        if cifar100_c_test[idx] is None:\n",
    "            cifar100_c_test[idx] = dataset\n",
    "        else:\n",
    "            cifar100_c_test[idx] = cifar100_c_test[idx].concatenate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics to watch during the evaluation in accordance to (Ovadia et. al, 2019)\n",
    "\n",
    "# Confidence\n",
    "confidence = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Categorical accuracy\n",
    "cat_acc = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Negative log-likelihood\n",
    "neg_log_likelihood = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Brier score\n",
    "brier_score = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Predictive entropy\n",
    "pred_entropy = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for severity in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    print('Severity: {}'.format(severity))\n",
    "\n",
    "    for idx, (imgs, ground_truths) in cifar100_c_test[severity].enumerate():\n",
    "        # Generate predictions for each image in the current batch from the coarse (parental)\n",
    "        batch_scores_coarse = coarse_model.predict(imgs)\n",
    "        \n",
    "        # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "        # version of the same base image, we can simply average the individual scores to get the final\n",
    "        # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "        # (Krizhevsky et al., 2012).\n",
    "        prediction_coarse = tf.math.reduce_mean(batch_scores_coarse, axis=0)\n",
    "        \n",
    "        # Route the current batch to the sub-module predcited by the coarse model and generate the\n",
    "        # predictions for the respective fine category labels\n",
    "        batch_scores_fine = fine_models[tf.math.argmax(prediction_coarse)].predict(imgs)\n",
    "        \n",
    "        # Average the scores for the individual images in the batch as described above\n",
    "        prediction_fine = tf.math.reduce_mean(batch_scores_fine, axis=0)\n",
    "        \n",
    "        # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "        # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "        \n",
    "        ground_truth = ground_truths[0]\n",
    "        ground_truth_max_idx = tf.math.argmax(ground_truth).numpy()\n",
    "\n",
    "        prediction = prediction_fine\n",
    "        prediction_max_idx = tf.math.argmax(prediction_fine).numpy()\n",
    "\n",
    "        confidence[severity].append(\n",
    "            prediction[prediction_max_idx])\n",
    "        \n",
    "        cat_acc[severity].append(tf.dtypes.cast(\n",
    "            tf.math.equal(ground_truth_max_idx, prediction_max_idx), tf.dtypes.float32))\n",
    "\n",
    "        neg_log_likelihood[severity].append(\n",
    "            -tf.math.log(prediction[ground_truth_max_idx]))\n",
    "\n",
    "        brier_score[severity].append(\n",
    "            tf.math.reduce_sum((prediction - ground_truth) ** 2))\n",
    "\n",
    "        pred_entropy[severity].append(\n",
    "            -tf.math.reduce_sum(tf.map_fn(lambda p: p * tf.math.log(p), prediction)))\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            print('{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "# Inflate the arrays in case only the uncorrupted test dataset was evaluated\n",
    "if CIFAR100_C_INCLUDE_UNCORRUPTED and not CIFAR100_C_CORRUPTIONS:\n",
    "    for _ in range(5):\n",
    "        confidence.append([])\n",
    "        cat_acc.append([])\n",
    "        neg_log_likelihood.append([])\n",
    "        brier_score.append([])\n",
    "        pred_entropy.append([])\n",
    "        \n",
    "# Append a placeholder for the uncorrupted dataset in case it was not evaluated\n",
    "if not CIFAR100_C_INCLUDE_UNCORRUPTED and CIFAR100_C_CORRUPTIONS:\n",
    "    for _ in range(5):\n",
    "        confidence.insert(0, [])\n",
    "        cat_acc.insert(0, [])\n",
    "        neg_log_likelihood.insert(0, [])\n",
    "        brier_score.insert(0, [])\n",
    "        pred_entropy.insert(0, [])\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'confidence_shifted.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in confidence]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'cat_acc.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in cat_acc]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'neg_log_likelihood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in neg_log_likelihood]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'brier_score.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in brier_score]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_COMPNET + 'pred_entropy_shifted.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in pred_entropy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "confidence = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_COMPNET + 'confidence_shifted.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "cat_acc = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_COMPNET + 'cat_acc.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "neg_log_likelihood = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_COMPNET + 'neg_log_likelihood.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "brier_score = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_COMPNET + 'brier_score.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "pred_entropy = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_COMPNET + 'pred_entropy_shifted.data'),\n",
    "            tf.dtypes.string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Categorical Accuracy over varying Corruption Severities', y=0.9225)\n",
    "\n",
    "bplot = ax.boxplot([val * 100 for val in cat_acc], sym='', showmeans= True, meanline=True, meanprops={'color':'C0'})\n",
    "\n",
    "ax.set_xlabel('Corruption severity')\n",
    "ax.set_ylabel('Accuracy in %')\n",
    "ax.set_xticklabels(range(CIFAR100_C_NUM_SEVERITY_GRADES))\n",
    "\n",
    "ax.legend([bplot[\"medians\"][0], bplot[\"means\"][0]], ['Median', 'Mean'], loc='upper right')\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_cat_acc.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Brier Score over varying Corruption Severities', y=0.9225)\n",
    "\n",
    "bplot = ax.boxplot(brier_score, sym='', showmeans= True, meanline=True, meanprops={'color':'C0'})\n",
    "\n",
    "ax.set_xlabel('Corruption severity')\n",
    "ax.set_ylabel('Brier Score')\n",
    "ax.set_xticklabels(range(CIFAR100_C_NUM_SEVERITY_GRADES))\n",
    "\n",
    "ax.legend([bplot[\"medians\"][0], bplot[\"means\"][0]], ['Median', 'Mean'], loc='upper right')\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_brier_score.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Confidence over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist(confidence[idx] * 100, bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Confidence ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_ylabel('Number of examples P(x) > ' +  r'$\\tau$')\n",
    "    ax[row][col].set_xlim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_confidence.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Negative Log-likelihood over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist([min(val, 50) for val in neg_log_likelihood[idx]], bins=10000, cumulative=1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Log-likelihood ' + r'$l$')\n",
    "    ax[row][col].set_ylabel('Number of examples L(x) > ' +  r'$l$')\n",
    "    ax[row][col].set_xlim([0, 50])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_neg_log_likelihood.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Predictive Entropy over varying Severity Grades', y=0.9109)\n",
    "\n",
    "# Filter out NaN entries from `pred_entropy`\n",
    "for idx, severity in enumerate(pred_entropy):\n",
    "    pred_entropy[idx] = tf.where(tf.math.is_nan(severity), tf.zeros_like(severity), severity)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist(pred_entropy[idx], bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Entropy ' + r'$h$' + ' in nats')\n",
    "    ax[row][col].set_ylabel('Number of examples H(x) > ' +  r'$h$')\n",
    "    ax[row][col].set_xlim([0, 2])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_pred_entropy.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 10000\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Confidence vs Accuracy over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "\n",
    "    if idx == 0:\n",
    "        window_size = int(WINDOW_SIZE / 10)\n",
    "    else:\n",
    "        window_size = WINDOW_SIZE\n",
    "\n",
    "    # Sort confidence and categorical accuracy in ascending order\n",
    "    confidence_sorted = [val * 100 for val in sorted(confidence[idx])]\n",
    "    cat_acc_sorted = [val for _, val in sorted(zip(confidence[idx], cat_acc[idx]))]\n",
    "\n",
    "    # Calculate the moving averages of the sorted categorical accuracy\n",
    "    cat_acc_cumsum = tf.math.cumsum(cat_acc_sorted)\n",
    "    cat_acc_moving_avgs = (cat_acc_cumsum[window_size:] - cat_acc_cumsum[:-window_size]) / window_size * 100\n",
    "\n",
    "    ax[row][col].plot(confidence_sorted[window_size:], cat_acc_moving_avgs)\n",
    "    \n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Confidence > ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_ylabel('Accuracy on examples P(x) > ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_xlim([0, 100])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ds_confidence_accuracy.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-distribution (OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test = process_and_augment(cifar10_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics to watch during the evaluation in accordance to (Ovadia et. al, 2019)\n",
    "# Annotation: As there is no ground truth for fully OOD samples, we only report confidence and\n",
    "# predictive entropy for those.\n",
    "\n",
    "# Confidence\n",
    "confidence = [0] * CIFAR10_NUM_TEST_SAMPLES\n",
    "\n",
    "# Predictive entropy\n",
    "pred_entropy = [0] * CIFAR10_NUM_TEST_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (imgs, ground_truths) in cifar10_test.enumerate():\n",
    "    # Generate predictions for each image in the current batch from the coarse (parental)\n",
    "    batch_scores_coarse = coarse_model.predict(imgs)\n",
    "    \n",
    "    # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "    # version of the same base image, we can simply average the individual scores to get the final\n",
    "    # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky et al., 2012).\n",
    "    prediction_coarse = tf.math.reduce_mean(batch_scores_coarse, axis=0)\n",
    "    \n",
    "    # Route the current batch to the sub-module predcited by the coarse model and generate the\n",
    "    # predictions for the respective fine category labels\n",
    "    batch_scores_fine = fine_models[tf.math.argmax(prediction_coarse)].predict(imgs)\n",
    "    \n",
    "    # Average the scores for the individual images in the batch as described above\n",
    "    prediction_fine = tf.math.reduce_mean(batch_scores_fine, axis=0)\n",
    "    \n",
    "    # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "    # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "    \n",
    "    ground_truth = ground_truths[0]\n",
    "    ground_truth_max_idx = tf.math.argmax(ground_truths).numpy()\n",
    "\n",
    "    prediction = prediction_fine\n",
    "    prediction_max_idx = tf.math.argmax(prediction_fine).numpy()\n",
    "\n",
    "    confidence[idx] = prediction[prediction_max_idx]\n",
    "\n",
    "    pred_entropy[idx] = -tf.math.reduce_sum(tf.map_fn(lambda p: p * tf.math.log(p), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_ood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in confidence]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_ood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in pred_entropy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "confidence = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_ood.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "pred_entropy = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_ood.data'),\n",
    "            tf.dtypes.string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Confidence on OOD Data', y=0.9225)\n",
    "\n",
    "ax.hist([val * 100 for val in confidence], bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "ax.set_xlabel('Confidence ' + r'$\\tau$' + ' in %')\n",
    "ax.set_ylabel('Number of examples P(x) > ' +  r'$\\tau$')\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ood_confidence.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Predictive Entropy on OOD Data', y=0.9225)\n",
    "\n",
    "# Filter out NaN entries from `pred_entropy`\n",
    "pred_entropy = tf.where(tf.math.is_nan(pred_entropy), tf.zeros_like(pred_entropy), pred_entropy)\n",
    "\n",
    "ax.hist(pred_entropy, bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "ax.set_xlabel('Entropy ' + r'$h$' + ' in nats')\n",
    "ax.set_ylabel('Number of examples H(x) > ' +  r'$h$')\n",
    "ax.set_xlim([0, 2])\n",
    "\n",
    "fig.savefig('uncertainty_compnet_ood_pred_entropy.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark: Maxout Network (Goodfellow et. al, 2013)\n",
    "<a id='benchmark'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparative benchmark, a (monolithic) Maxout Network as described in (Goodfellow et. al, 2013) was trained on the basis of the same preprocessed dataset. Hyperparameters were tuned experimentally as there is - to the best of my knowledge - no information available as to what configuration was used to achieve the results reported in the publication.\n",
    "\n",
    "Criteria that were taken into account when selecting the benchmark network's architecture:\n",
    "- a) Simplicity / Leanness (i.e. no inferred knowledge in the architecture for the same [reasons](#compnet) mentioned when arguing about the composed model's architecture)\n",
    "- b) Existence of reference values in literature\n",
    "- c) Scalability (so that we could use a downscaled version for the composed model for comparative reasons)\n",
    "- d) Contemporary performance levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "<a id='benchmark_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [above](#compnet_model_maxout) with regard to the construction of MaxOut feature maps and the topic native API vs custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We imply a combination of Dropout and MaxNorm for weight regularization as is reported\n",
    "# in (Srivastava et al., 2014) to be most effective. Furthermore, we also apply Dropout to the\n",
    "# convolutional layers to facilitate the discovery of informative features as described in\n",
    "# (Park et al., 2016).\n",
    "\n",
    "# Input specification\n",
    "inputs = tf.keras.Input(shape=(28, 28, 3))\n",
    "\n",
    "# First convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=128,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           activation=None,\n",
    "                           use_bias=True,\n",
    "                           kernel_constraint='max_norm',\n",
    "                           bias_constraint='max_norm')(inputs)\n",
    "x = tf.keras.layers.Reshape((28, 28, 128, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                              strides=(1, 1, 4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((28, 28, 32))(x)\n",
    "\n",
    "# First MaxPool layer\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              padding='same')(x)\n",
    "\n",
    "# First Dropout layer\n",
    "#x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "# Second convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=256,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           activation=None,\n",
    "                           use_bias=True,\n",
    "                           kernel_constraint='max_norm',\n",
    "                           bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((14, 14, 256, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                              strides=(1, 1, 4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((14, 14, 64))(x)\n",
    "\n",
    "# Second MaxPool layer\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              padding='same')(x)\n",
    "\n",
    "# Second Dropout layer\n",
    "#x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "# Third convolutional MaxOut layer\n",
    "x = tf.keras.layers.Conv2D(filters=512,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           activation=None,\n",
    "                           use_bias=True,\n",
    "                           kernel_constraint='max_norm',\n",
    "                           bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((7, 7, 512, 1))(x)\n",
    "x = tf.keras.layers.MaxPool3D(pool_size=(1, 1, 4),\n",
    "                              strides=(1, 1, 4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Reshape((7, 7, 128))(x)\n",
    "\n",
    "# Transition between convolutional and fully connected layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Third Dropout layer\n",
    "#x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Fully connected MaxOut layer\n",
    "x = tf.keras.layers.Dense(units=512,\n",
    "                          activation=None,\n",
    "                          use_bias=True,\n",
    "                          kernel_constraint='max_norm',\n",
    "                          bias_constraint='max_norm')(x)\n",
    "x = tf.keras.layers.Reshape((512, 1))(x)\n",
    "x = x = tf.keras.layers.MaxPool1D(pool_size=(4),\n",
    "                              strides=(4),\n",
    "                              padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Fourth Dropout layer\n",
    "x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Softmax output layer\n",
    "outputs = tf.keras.layers.Dense(units=CIFAR100_NUM_FINE_LABELS,\n",
    "                                activation='softmax',\n",
    "                                use_bias=True,\n",
    "                                kernel_constraint='max_norm',\n",
    "                                bias_constraint='max_norm')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Categorical Cross Entropy as loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Define metrics to watch during training\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "           tf.keras.metrics.CategoricalCrossentropy(),\n",
    "           tf.keras.metrics.AUC()]\n",
    "\n",
    "# Use Adam (Kingma et al., 2017) as optimizer during training\n",
    "# Annotation: We don't set `learning_rate` here as this is automatically handled by the\n",
    "# LearningRateScheduler (cf. callback section below).\n",
    "optimizer = tf.keras.optimizers.Adam(beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: Restore model from checkpoint\n",
    "# model = tf.keras.models.load_model(CKPT_DIR + 'maxout')\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id='benchmark_train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_train = process_and_augment(cifar100_train_raw, batch_size=32, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1)\n",
    "cifar100_val = process_and_augment(cifar100_val_raw, batch_size=32, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=True, num_rnd_crops=5, shuffle_buffer_size=1000, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping: Stop training early if no significant improvement in the monitored quantity is\n",
    "#     observed for at least `patience` epochs\n",
    "# LearningRateScheduler: Dynamically adapt the learning rate depending on the training epoch to\n",
    "#     facilitate accelerated learning during the first few epochs\n",
    "# ModelCheckpoint: Save the model after each epoch (if `save_best_only` is set to True, only keep\n",
    "#     the best model with regard to the monitored quantity)\n",
    "# TensorBoard: Enable TensorBoard visualization\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=0.01,\n",
    "                                              patience=3),\n",
    "             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-03 if epoch < 3 else 1e-04),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_DIR + 'maxout',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=False,\n",
    "                                                save_best_only=True),\n",
    "             tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
    "                                            histogram_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=cifar100_train,\n",
    "          epochs=100,\n",
    "          verbose=True,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=cifar100_val,\n",
    "          shuffle=True,\n",
    "          validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "<a id='benchmark_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform model evaluation adhering to the standard 10-crop procedure as described in (Goodfellow et al., 2013) and (Krizhevsky et al., 2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_test = process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to watch during the evaluation of the model on the test data set\n",
    "\n",
    "# Use the same metrics as for the training\n",
    "# test_metrics = metrics\n",
    "\n",
    "# Use different metrics than during the training\n",
    "test_metrics = [tf.keras.metrics.CategoricalAccuracy(name='CategoricalAccuracy'),\n",
    "                tf.keras.metrics.CategoricalCrossentropy(name='CategoricalCrossentropy'),\n",
    "                tf.keras.metrics.AUC(name='AUC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all metrics before starting the evaluation\n",
    "for metric in test_metrics:\n",
    "    metric.reset_states()\n",
    "    \n",
    "# Initialize additional custom metrics to watch during the evaluation\n",
    "\n",
    "# Overall label distribution (predicted and ground truth)\n",
    "dist_ground_truth = [0] * CIFAR100_NUM_FINE_LABELS\n",
    "dist_predicted = [0] * CIFAR100_NUM_FINE_LABELS\n",
    "\n",
    "# Predicted label distribution for each coarse category\n",
    "dist_predicted_coarse = [[0] * CIFAR100_NUM_COARSE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "\n",
    "# Predicted label distribution for each fine category\n",
    "dist_predicted_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_FINE_LABELS)]\n",
    "\n",
    "# Fine label distribution for each coarse category (predicted and ground truth)\n",
    "dist_ground_truth_coarse_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "dist_predicted_coarse_fine = [[0] * CIFAR100_NUM_FINE_LABELS for _ in range(CIFAR100_NUM_COARSE_LABELS)]\n",
    "\n",
    "# Semantic distance between the predicted category and the ground truth in accordance to\n",
    "# (Fergus et al., 2010) \n",
    "semantic_distance = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (imgs, ground_truths) in cifar100_test:\n",
    "    # Generate predictions for each image in the current batch\n",
    "    batch_scores = model.predict(imgs)\n",
    "    \n",
    "    # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "    # version of the same base image, we can simply average the individual scores to get the final\n",
    "    # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky et al., 2012).\n",
    "    prediction = tf.math.reduce_mean(batch_scores, axis=0)\n",
    "    \n",
    "    # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "    # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "    for metric in test_metrics:\n",
    "        metric.update_state(ground_truths[0], prediction)\n",
    "        \n",
    "    # Update custom metrics w/ the result for the current base image; cf. above concerning the\n",
    "    # ground truth for each batch\n",
    "    \n",
    "    ground_truth_fine = tf.math.argmax(ground_truths[0]).numpy()\n",
    "    ground_truth_fine_decoded = cifar100_decode(ground_truth_fine, CIFAR100_FINE_LABEL_LEVEL)\n",
    "    \n",
    "    ground_truth_coarse = cifar100_resolve_hypernym(ground_truth_fine, level=CIFAR100_COARSE_LABEL_LEVEL, encoded=True)\n",
    "        \n",
    "    prediction_fine = tf.math.argmax(prediction).numpy()\n",
    "    prediction_fine_decoded = cifar100_decode(prediction_fine, CIFAR100_FINE_LABEL_LEVEL)\n",
    "    \n",
    "    prediction_coarse = cifar100_resolve_hypernym(prediction_fine, level=CIFAR100_COARSE_LABEL_LEVEL, encoded=True)\n",
    "    \n",
    "    dist_ground_truth[ground_truth_fine] += 1\n",
    "    dist_predicted[prediction_fine] += 1\n",
    "\n",
    "    dist_predicted_coarse[ground_truth_coarse][prediction_coarse] += 1\n",
    "\n",
    "    dist_predicted_fine[ground_truth_fine][prediction_fine] += 1\n",
    "\n",
    "    dist_ground_truth_coarse_fine[ground_truth_coarse][ground_truth_fine] += 1\n",
    "    dist_predicted_coarse_fine[ground_truth_coarse][prediction_fine] += 1\n",
    "    \n",
    "    semantic_distance += CIFAR100_SYNSET_MAP.semantic_distance(\n",
    "        ground_truth_fine_decoded,\n",
    "        prediction_fine_decoded\n",
    "    )\n",
    "\n",
    "print('Benchmark')\n",
    "print()\n",
    "print('==================================================')\n",
    "print()\n",
    "print('Final results:')\n",
    "for metric in test_metrics:\n",
    "    print('{}: {}'.format(metric.name, metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_ground_truth.data',\n",
    "    tf.io.serialize_tensor(dist_ground_truth))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted.data',\n",
    "    tf.io.serialize_tensor(dist_predicted))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_coarse.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_coarse))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_fine.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_ground_truth_coarse_fine.data',\n",
    "    tf.io.serialize_tensor(dist_ground_truth_coarse_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_coarse_fine.data',\n",
    "    tf.io.serialize_tensor(dist_predicted_coarse_fine))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'semantic_distance.data',\n",
    "    tf.io.serialize_tensor(semantic_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "dist_ground_truth = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_ground_truth.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_coarse = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_coarse.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_ground_truth_coarse_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_ground_truth_coarse_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "dist_predicted_coarse_fine = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'dist_predicted_coarse_fine.data'),\n",
    "        tf.dtypes.int64)\n",
    "\n",
    "semantic_distance = tf.io.parse_tensor(\n",
    "    tf.io.read_file(\n",
    "        CIFAR100_RESULTS_DIR_BENCHMARK + 'semantic_distance.data'),\n",
    "        tf.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(21, 5.5))\n",
    "fig.suptitle('Predicted vs Ground Truth Distribution')\n",
    "\n",
    "titles = ['Predicted', 'Ground Truth']\n",
    "\n",
    "for idx, dist in enumerate([dist_predicted, dist_ground_truth]):\n",
    "    # Normalize the distribution\n",
    "    dist = list(map(lambda entry: entry / sum(dist) * 100, dist)) \n",
    "                \n",
    "    # Plot the distribution\n",
    "    ax[idx].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[idx].set_title(titles[idx])\n",
    "    ax[idx].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "    ax[idx].set_xlim([0.5, 100.5])\n",
    "    ax[idx].set_ylim([0, 3.3])\n",
    "    ax[idx].set_xlabel('Label')\n",
    "    ax[idx].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_benchmark_dist_pred_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 2, figsize=(21, 65.5))\n",
    "fig.suptitle('Predicted Coarse Distribution per Coarse Ground Truth', y=0.88885)\n",
    "\n",
    "for label in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    row = int(label / 2)\n",
    "    col = label % 2\n",
    "\n",
    "    # Normalize the distribution\n",
    "    dist = list(map(\n",
    "        lambda entry: entry / sum(dist_predicted_coarse[label]) * 100, dist_predicted_coarse[label])) \n",
    "\n",
    "    # Plot the distribution\n",
    "    ax[row][col].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[row][col].set_title(\n",
    "        '{}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL))\n",
    "    )\n",
    "    ax[row][col].set_xticks(range(0, CIFAR100_NUM_COARSE_LABELS, 2))\n",
    "    ax[row][col].set_xlim([0.5, 20.5])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "    ax[row][col].set_xlabel('Label')\n",
    "    ax[row][col].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_benchmark_dist_pred_coarse_per_coarse_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(50, 2, figsize=(21, 330.5))\n",
    "fig.suptitle('Predicted Fine Distribution per Fine Ground Truth', y=0.88175)\n",
    "\n",
    "for label in range(CIFAR100_NUM_FINE_LABELS):\n",
    "    row = int(label / 2)\n",
    "    col = int(label % 2)\n",
    "    \n",
    "    # Normalize the distribution\n",
    "    dist = list(map(\n",
    "        lambda entry: entry / sum(dist_predicted_fine[label]) * 100, dist_predicted_fine[label])) \n",
    "\n",
    "    # Plot the distribution\n",
    "    ax[row][col].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "    ax[row][col].set_title(\n",
    "        '{}'.format(cifar100_decode(label, CIFAR100_FINE_LABEL_LEVEL))\n",
    "    )\n",
    "    ax[row][col].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "    ax[row][col].set_xlim([0.5, 100.5])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "    ax[row][col].set_xlabel('Label')\n",
    "    ax[row][col].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_benchmark_dist_pred_fine_per_fine_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, 2, figsize=(21, 130.5))\n",
    "fig.suptitle('Predicted vs Ground Truth Fine Distribution per Coarse Ground Truth', y=0.88425)\n",
    "\n",
    "for label in range(CIFAR100_NUM_COARSE_LABELS):\n",
    "    titles = ['Predicted for {}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL)),\n",
    "          'Ground Truth for {}'.format(cifar100_decode(label, CIFAR100_COARSE_LABEL_LEVEL))]\n",
    "\n",
    "    for idx, dist in enumerate([dist_predicted_coarse_fine, dist_ground_truth_coarse_fine]):\n",
    "        # Normalize the distribution\n",
    "        dist = list(map(lambda entry: entry / sum(dist[label]) * 100, dist[label])) \n",
    "                    \n",
    "        # Plot the distribution\n",
    "        ax[label][idx].bar(range(1, len(dist) + 1), dist, width=1)\n",
    "        ax[label][idx].set_title(titles[idx])\n",
    "        ax[label][idx].set_xticks(range(0, CIFAR100_NUM_FINE_LABELS, 10))\n",
    "        ax[label][idx].set_xlim([0.5, 100.5])\n",
    "        ax[label][idx].set_ylim([0, 50])\n",
    "        ax[label][idx].set_xlabel('Label')\n",
    "        ax[label][idx].set_ylabel('Share in %')\n",
    "\n",
    "fig.savefig('cifar100_benchmark_dist_pred_groundTruth_fine_per_coarse_groundTruth.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive uncertainty under dataset shift\n",
    "<a id='benchmark_predictive_uncertainty'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examine how the benchmark model behaves with respect to predictive uncertainty under dataset shift. Methodologically, we follow the basic approach described by (Ovadia et. al, 2019) and, in doing so, employ for the shifted in-distribution data the corrupted CIFAR100-C dataset (Hendrycks et al., 2019) resp. the CIFAR10 dataset as out-of-distribution (OOD) reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR100-C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specific configuration\n",
    "\n",
    "# Storage location of the CIFAR100-C dataset\n",
    "CIFAR100_C_DATA_DIR = TFDS_DATA_DIR + 'cifar100/corrupted/'\n",
    "\n",
    "# List of corruption types to include in the dataset on load\n",
    "# Annotation: For a comprehensive list of all corruption types, cf. https://github.com/hendrycks/robustness).\n",
    "CIFAR100_C_CORRUPTIONS = [\n",
    "    'defocus_blur',\n",
    "    'gaussian_blur',\n",
    "    'glass_blur',\n",
    "    'motion_blur',\n",
    "    'zoom_blur',\n",
    "    'gaussian_noise',\n",
    "    'impulse_noise',\n",
    "    'shot_noise',\n",
    "    'speckle_noise',\n",
    "    'fog',\n",
    "    'frost',\n",
    "    'snow',\n",
    "    'brightness',\n",
    "    'contrast',\n",
    "    'saturate',\n",
    "    'elastic_transform',\n",
    "    'jpeg_compression',\n",
    "    'pixelate',\n",
    "    'spatter'\n",
    "]\n",
    "\n",
    "# Indicator whether to include the uncorrupted dataset for reference in addition to the above\n",
    "# corruptions during the subsequent evaluation\n",
    "CIFAR100_C_INCLUDE_UNCORRUPTED = True\n",
    "\n",
    "# Number of severity grades and respectively associated samples per corruption\n",
    "CIFAR100_C_NUM_SEVERITY_GRADES = (5 * (1 if CIFAR100_C_CORRUPTIONS else 0) + CIFAR100_C_INCLUDE_UNCORRUPTED)\n",
    "CIFAR100_C_NUM_SAMPLES_PER_SEVERITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR100-C dataset\n",
    "\n",
    "# Template of the structure of the individual records to apply when parsing the serialized dataset\n",
    "record_template = {\n",
    "    CIFAR100_IMG_KEY: tf.io.FixedLenFeature([], tf.dtypes.string, default_value=''),\n",
    "    CIFAR100_FINE_LABEL_KEY: tf.io.FixedLenFeature([], tf.dtypes.int64, default_value=-1)\n",
    "}\n",
    "\n",
    "# Load the dataset including all corruptions specified by `CIFAR100_C_CORRUPTIONS`\n",
    "cifar100_c_test_raw = tf.data.TFRecordDataset([CIFAR100_C_DATA_DIR + corruption + '.tfrecord' for corruption in CIFAR100_C_CORRUPTIONS])\n",
    "\n",
    "# Parse the serialized `tf.train.Example` protos\n",
    "cifar100_c_test_raw = cifar100_c_test_raw.map(\n",
    "    lambda record: tf.io.parse_single_example(record, record_template),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# JPEG-decode the images\n",
    "cifar100_c_test_raw = cifar100_c_test_raw.map(\n",
    "    lambda record: {CIFAR100_IMG_KEY: tf.io.decode_jpeg(record[CIFAR100_IMG_KEY]),\n",
    "                    CIFAR100_FINE_LABEL_KEY: record[CIFAR100_FINE_LABEL_KEY]},\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specific configuration\n",
    "\n",
    "# Number of samples in the test dataset\n",
    "CIFAR10_NUM_TEST_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test_raw, cifar10_info = tfds.load('cifar10',\n",
    "                                           split='test',\n",
    "                                           data_dir=TFDS_DATA_DIR,\n",
    "                                           download_and_prepare_kwargs={'download_dir': TFDS_DOWNLOAD_DIR},\n",
    "                                           with_info=True)\n",
    "print(cifar10_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributional shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_c_test = [None] * CIFAR100_C_NUM_SEVERITY_GRADES\n",
    "\n",
    "if CIFAR100_C_INCLUDE_UNCORRUPTED:\n",
    "    # Include the uncorrupted test dataset for reference\n",
    "    cifar100_c_test[0] = process_and_augment(cifar100_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)\n",
    "\n",
    "# Construct one test dataset per severity grade\n",
    "for corruption in range(len(CIFAR100_C_CORRUPTIONS)):\n",
    "    print('Corruption: {}'.format(corruption))\n",
    "\n",
    "    for severity in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "        print('Severity: {}'.format(severity))\n",
    "\n",
    "        # As the corruption files are read in consecutively and each file contains\n",
    "        # `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` * `CIFAR100_C_NUM_SEVERITY_GRADES` records with the\n",
    "        # first `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` samples being of the lowest severity grade\n",
    "        # followed by `CIFAR100_C_NUM_SAMPLES_PER_SEVERITY` samples of the secont severity grade, etc.,\n",
    "        # we can construct a dataset containing only samples of corruption `corruption` and severity\n",
    "        # grade `severity` as follows:\n",
    "        dataset = cifar100_c_test_raw.enumerate().filter(\n",
    "            lambda idx, _: int(idx / CIFAR100_C_NUM_SAMPLES_PER_SEVERITY) == corruption * (CIFAR100_C_NUM_SEVERITY_GRADES - CIFAR100_C_INCLUDE_UNCORRUPTED) + severity)\n",
    "        \n",
    "        # Remove the index residue from the previous filtering operation\n",
    "        dataset = dataset.map(\n",
    "            lambda _, record: record,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        dataset = process_and_augment(dataset, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)\n",
    "        \n",
    "        idx = severity + CIFAR100_C_INCLUDE_UNCORRUPTED\n",
    "        if cifar100_c_test[idx] is None:\n",
    "            cifar100_c_test[idx] = dataset\n",
    "        else:\n",
    "            cifar100_c_test[idx] = cifar100_c_test[idx].concatenate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics to watch during the evaluation in accordance to (Ovadia et. al, 2019)\n",
    "\n",
    "# Confidence\n",
    "confidence = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Categorical accuracy\n",
    "cat_acc = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Negative log-likelihood\n",
    "neg_log_likelihood = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Brier score\n",
    "brier_score = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]\n",
    "\n",
    "# Predictive entropy\n",
    "pred_entropy = [[] for _ in range(CIFAR100_C_NUM_SEVERITY_GRADES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for severity in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    print('Severity: {}'.format(severity))\n",
    "\n",
    "    for idx, (imgs, ground_truths) in cifar100_c_test[severity].enumerate():\n",
    "        # Generate predictions for each image in the current batch\n",
    "        batch_scores = model.predict(imgs)\n",
    "        \n",
    "        # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "        # version of the same base image, we can simply average the individual scores to get the final\n",
    "        # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "        # (Krizhevsky et al., 2012).\n",
    "        prediction = tf.math.reduce_mean(batch_scores, axis=0)\n",
    "        \n",
    "        # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "        # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "        \n",
    "        ground_truth = ground_truths[0]\n",
    "        ground_truth_max_idx = tf.math.argmax(ground_truth).numpy()\n",
    "\n",
    "        prediction_max_idx = tf.math.argmax(prediction).numpy()\n",
    "\n",
    "        confidence[severity].append(\n",
    "            prediction[prediction_max_idx])\n",
    "        \n",
    "        cat_acc[severity].append(tf.dtypes.cast(\n",
    "            tf.math.equal(ground_truth_max_idx, prediction_max_idx), tf.dtypes.float32))\n",
    "\n",
    "        neg_log_likelihood[severity].append(\n",
    "            -tf.math.log(prediction[ground_truth_max_idx]))\n",
    "\n",
    "        brier_score[severity].append(\n",
    "            tf.math.reduce_sum((prediction - ground_truth) ** 2))\n",
    "\n",
    "        pred_entropy[severity].append(\n",
    "            -tf.math.reduce_sum(tf.map_fn(lambda p: p * tf.math.log(p), prediction)))\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            print('{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "# Inflate the arrays in case only the uncorrupted test dataset was evaluated\n",
    "if CIFAR100_C_INCLUDE_UNCORRUPTED and not CIFAR100_C_CORRUPTIONS:\n",
    "    for _ in range(5):\n",
    "        confidence.append([])\n",
    "        cat_acc.append([])\n",
    "        neg_log_likelihood.append([])\n",
    "        brier_score.append([])\n",
    "        pred_entropy.append([])\n",
    "        \n",
    "# Append a placeholder for the uncorrupted dataset in case it was not evaluated\n",
    "if not CIFAR100_C_INCLUDE_UNCORRUPTED and CIFAR100_C_CORRUPTIONS:\n",
    "    for _ in range(5):\n",
    "        confidence.insert(0, [])\n",
    "        cat_acc.insert(0, [])\n",
    "        neg_log_likelihood.insert(0, [])\n",
    "        brier_score.insert(0, [])\n",
    "        pred_entropy.insert(0, [])\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_shifted.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in confidence]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'cat_acc.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in cat_acc]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'neg_log_likelihood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in neg_log_likelihood]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'brier_score.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in brier_score]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_shifted.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in pred_entropy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "confidence = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_shifted.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "cat_acc = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'cat_acc.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "neg_log_likelihood = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'neg_log_likelihood.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "brier_score = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'brier_score.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "pred_entropy = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_shifted.data'),\n",
    "            tf.dtypes.string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Categorical Accuracy over varying Corruption Severities', y=0.9225)\n",
    "\n",
    "bplot = ax.boxplot([val * 100 for val in cat_acc], sym='', showmeans= True, meanline=True, meanprops={'color':'C0'})\n",
    "\n",
    "ax.set_xlabel('Corruption severity')\n",
    "ax.set_ylabel('Accuracy in %')\n",
    "ax.set_xticklabels(range(CIFAR100_C_NUM_SEVERITY_GRADES))\n",
    "\n",
    "ax.legend([bplot[\"medians\"][0], bplot[\"means\"][0]], ['Median', 'Mean'], loc='upper right')\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_cat_acc.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Brier Score over varying Corruption Severities', y=0.9225)\n",
    "\n",
    "bplot = ax.boxplot(brier_score, sym='', showmeans= True, meanline=True, meanprops={'color':'C0'})\n",
    "\n",
    "ax.set_xlabel('Corruption severity')\n",
    "ax.set_ylabel('Brier Score')\n",
    "ax.set_xticklabels(range(CIFAR100_C_NUM_SEVERITY_GRADES))\n",
    "\n",
    "ax.legend([bplot[\"medians\"][0], bplot[\"means\"][0]], ['Median', 'Mean'], loc='upper right')\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_brier_score.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Confidence over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist(confidence[idx] * 100, bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Confidence ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_ylabel('Number of examples P(x) > ' +  r'$\\tau$')\n",
    "    ax[row][col].set_xlim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_confidence.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Negative Log-likelihood over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist([min(val, 10) for val in neg_log_likelihood[idx]], bins=10000, cumulative=1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Log-likelihood ' + r'$l$')\n",
    "    ax[row][col].set_ylabel('Number of examples L(x) > ' +  r'$l$')\n",
    "    ax[row][col].set_xlim([0, 10])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_neg_log_likelihood.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Predictive Entropy over varying Severity Grades', y=0.9109)\n",
    "\n",
    "# Filter out NaN entries from `pred_entropy`\n",
    "for idx, severity in enumerate(pred_entropy):\n",
    "    pred_entropy[idx] = tf.where(tf.math.is_nan(severity), tf.zeros_like(severity), severity)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax[row][col].hist(pred_entropy[idx], bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Entropy ' + r'$h$' + ' in nats')\n",
    "    ax[row][col].set_ylabel('Number of examples H(x) > ' +  r'$h$')\n",
    "    ax[row][col].set_xlim([0, 5])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_pred_entropy.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 10000\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(21, 19))\n",
    "fig.suptitle('Confidence vs Accuracy over varying Severity Grades', y=0.9109)\n",
    "\n",
    "for idx in range(CIFAR100_C_NUM_SEVERITY_GRADES):\n",
    "    row = int(idx / 2)\n",
    "    col = idx % 2\n",
    "\n",
    "    if idx == 0:\n",
    "        window_size = int(WINDOW_SIZE / 10)\n",
    "    else:\n",
    "        window_size = WINDOW_SIZE\n",
    "\n",
    "    # Sort confidence and categorical accuracy in ascending order\n",
    "    confidence_sorted = [val * 100 for val in sorted(confidence[idx])]\n",
    "    cat_acc_sorted = [val for _, val in sorted(zip(confidence[idx], cat_acc[idx]))]\n",
    "\n",
    "    # Calculate the moving averages of the sorted categorical accuracy\n",
    "    cat_acc_cumsum = tf.math.cumsum(cat_acc_sorted)\n",
    "    cat_acc_moving_avgs = (cat_acc_cumsum[window_size:] - cat_acc_cumsum[:-window_size]) / window_size * 100\n",
    "\n",
    "    ax[row][col].plot(confidence_sorted[window_size:], cat_acc_moving_avgs)\n",
    "    \n",
    "    ax[row][col].set_title('Severity Grade {}'.format(idx))\n",
    "    ax[row][col].set_xlabel('Confidence > ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_ylabel('Accuracy on examples P(x) > ' + r'$\\tau$' + ' in %')\n",
    "    ax[row][col].set_xlim([0, 100])\n",
    "    ax[row][col].set_ylim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ds_confidence_accuracy.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-distribution (OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test = process_and_augment(cifar10_test_raw, batch_size=10, synset_level=CIFAR100_FINE_LABEL_LEVEL, hypernym=None, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics to watch during the evaluation in accordance to (Ovadia et. al, 2019)\n",
    "# Annotation: As there is no ground truth for fully OOD samples, we only report confidence and\n",
    "# predictive entropy for those.\n",
    "\n",
    "# Confidence\n",
    "confidence = [0] * CIFAR10_NUM_TEST_SAMPLES\n",
    "\n",
    "# Predictive entropy\n",
    "pred_entropy = [0] * CIFAR10_NUM_TEST_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (imgs, ground_truths) in cifar10_test.enumerate():\n",
    "    # Generate predictions for each image in the current batch\n",
    "    batch_scores = model.predict(imgs)\n",
    "    \n",
    "    # Since we constructed our test data set in a way that each image in one batch is an augmented\n",
    "    # version of the same base image, we can simply average the individual scores to get the final\n",
    "    # prediction for the respective base image in adherence to (Goodfellow et al., 2013) and\n",
    "    # (Krizhevsky et al., 2012).\n",
    "    prediction = tf.math.reduce_mean(batch_scores, axis=0)\n",
    "    \n",
    "    # Update the metrics w/ the result for the current base image; as all images in one batch\n",
    "    # originate from the same base image (cf. above), the ground truth is hence identical as well.\n",
    "    \n",
    "    ground_truth = ground_truths[0]\n",
    "    ground_truth_max_idx = tf.math.argmax(ground_truths).numpy()\n",
    "\n",
    "    prediction_max_idx = tf.math.argmax(prediction).numpy()\n",
    "\n",
    "    confidence[idx] = prediction[prediction_max_idx]\n",
    "\n",
    "    pred_entropy[idx] = -tf.math.reduce_sum(tf.map_fn(lambda p: p * tf.math.log(p), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_ood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in confidence]))\n",
    "\n",
    "tf.io.write_file(\n",
    "    CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_ood.data',\n",
    "    tf.io.serialize_tensor(\n",
    "        [tf.io.serialize_tensor(entry) for entry in pred_entropy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "\n",
    "confidence = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'confidence_ood.data'),\n",
    "            tf.dtypes.string)]\n",
    "\n",
    "pred_entropy = [tf.io.parse_tensor(\n",
    "    entry, tf.dtypes.float32) for entry in tf.io.parse_tensor(\n",
    "        tf.io.read_file(\n",
    "            CIFAR100_RESULTS_DIR_BENCHMARK + 'pred_entropy_ood.data'),\n",
    "            tf.dtypes.string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Confidence on OOD Data', y=0.9225)\n",
    "\n",
    "ax.hist([val * 100 for val in confidence], bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "ax.set_xlabel('Confidence ' + r'$\\tau$' + ' in %')\n",
    "ax.set_ylabel('Number of examples P(x) > ' +  r'$\\tau$')\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ood_confidence.jpg', format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(21, 7.5))\n",
    "fig.suptitle('Predictive Entropy on OOD Data', y=0.9225)\n",
    "\n",
    "# Filter out NaN entries from `pred_entropy`\n",
    "pred_entropy = tf.where(tf.math.is_nan(pred_entropy), tf.zeros_like(pred_entropy), pred_entropy)\n",
    "\n",
    "ax.hist(pred_entropy, bins=10000, cumulative=-1, histtype='step')\n",
    "\n",
    "ax.set_xlabel('Entropy ' + r'$h$' + ' in nats')\n",
    "ax.set_ylabel('Number of examples H(x) > ' +  r'$h$')\n",
    "ax.set_xlim([0, 5])\n",
    "\n",
    "fig.savefig('uncertainty_benchmark_ood_pred_entropy.jpg', format='jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_net (Python 3.6.9)",
   "language": "python",
   "name": "comp_net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
